{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 모델은 무엇인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1~4.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n",
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사전 코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------\n",
    "# データ等の準備\n",
    "# ----------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# train_xは学習データ、train_yは目的変数、test_xはテストデータ\n",
    "# pandasのDataFrame, Seriesで保持します。（numpyのarrayで保持することもあります）\n",
    "\n",
    "train = pd.read_csv('../input/sample-data/train_preprocessed.csv')\n",
    "train_x = train.drop(['target'], axis=1)\n",
    "train_y = train['target']\n",
    "test_x = pd.read_csv('../input/sample-data/test_preprocessed.csv')\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# コードの動作を確認するためのモデル\n",
    "class Model:\n",
    "\n",
    "    def __init__(self, params=None):\n",
    "        self.model = None\n",
    "        if params is None:\n",
    "            self.params = {}\n",
    "        else:\n",
    "            self.params = params\n",
    "\n",
    "    def fit(self, tr_x, tr_y):\n",
    "        params = {'objective': 'binary:logistic', 'silent': 1, 'random_state': 71}\n",
    "        params.update(self.params)\n",
    "        num_round = 10\n",
    "        dtrain = xgb.DMatrix(tr_x, label=tr_y)\n",
    "        self.model = xgb.train(params, dtrain, num_round)\n",
    "\n",
    "    def predict(self, x):\n",
    "        data = xgb.DMatrix(x)\n",
    "        pred = self.model.predict(data)\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -----------------------------------\n",
    "#### モデルの学習と予測\n",
    "#### -----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルのハイパーパラメータを指定する\n",
    "params = {'param1': 10, 'param2': 100}\n",
    "\n",
    "# Modelクラスを定義しているものとする\n",
    "# Modelクラスは、fitで学習し、predictで予測値の確率を出力する\n",
    "\n",
    "# モデルを定義する\n",
    "model = Model(params)\n",
    "\n",
    "# 学習データに対してモデルを学習させる\n",
    "model.fit(train_x, train_y)\n",
    "\n",
    "# テストデータに対して予測結果を出力する\n",
    "pred = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# バリデーション\n",
    "# -----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.3009\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# 学習データ・バリデーションデータを分けるためのインデックスを作成する\n",
    "# 学習データを4つに分割し、うち1つをバリデーションデータとする\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=71)\n",
    "tr_idx, va_idx = list(kf.split(train_x))[0]\n",
    "\n",
    "# 学習データを学習データとバリデーションデータに分ける\n",
    "tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n",
    "tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n",
    "\n",
    "# モデルを定義する\n",
    "model = Model(params)\n",
    "\n",
    "# 学習データに対してモデルを学習させる\n",
    "# モデルによっては、バリデーションデータを同時に与えてスコアをモニタリングすることができる\n",
    "model.fit(tr_x, tr_y)\n",
    "\n",
    "# バリデーションデータに対して予測し、評価を行う\n",
    "va_pred = model.predict(va_x)\n",
    "score = log_loss(va_y, va_pred)\n",
    "print(f'logloss: {score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# クロスバリデーション\n",
    "# -----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.2967\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# 学習データを4つに分け、うち1つをバリデーションデータとする\n",
    "# どれをバリデーションデータとするかを変えて学習・評価を4回行う\n",
    "scores = []\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=71)\n",
    "for tr_idx, va_idx in kf.split(train_x):\n",
    "    tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n",
    "    tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n",
    "    model = Model(params)\n",
    "    model.fit(tr_x, tr_y)\n",
    "    va_pred = model.predict(va_x)\n",
    "    score = log_loss(va_y, va_pred)\n",
    "    scores.append(score)\n",
    "\n",
    "# クロスバリデーションの平均のスコアを出力する\n",
    "print(f'logloss: {np.mean(scores):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ch04-02-run_xgb.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 분석 대회에 사용되는 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------\n",
    "# データ等の準備\n",
    "# ----------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# train_xは学習データ、train_yは目的変数、test_xはテストデータ\n",
    "# pandasのDataFrame, Seriesで保持します。（numpyのarrayで保持することもあります）\n",
    "\n",
    "train = pd.read_csv('../input/sample-data/train_preprocessed.csv')\n",
    "train_x = train.drop(['target'], axis=1)\n",
    "train_y = train['target']\n",
    "test_x = pd.read_csv('../input/sample-data/test_preprocessed.csv')\n",
    "\n",
    "# 学習データを学習データとバリデーションデータに分ける\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=71)\n",
    "tr_idx, va_idx = list(kf.split(train_x))[0]\n",
    "tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n",
    "tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.4  xgboostの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-error:0.128533\teval-error:0.1516\n",
      "[1]\ttrain-error:0.115333\teval-error:0.146\n",
      "[2]\ttrain-error:0.109333\teval-error:0.1376\n",
      "[3]\ttrain-error:0.105333\teval-error:0.1364\n",
      "[4]\ttrain-error:0.096933\teval-error:0.1384\n",
      "[5]\ttrain-error:0.094667\teval-error:0.1364\n",
      "[6]\ttrain-error:0.087333\teval-error:0.1296\n",
      "[7]\ttrain-error:0.084933\teval-error:0.1244\n",
      "[8]\ttrain-error:0.078133\teval-error:0.1208\n",
      "[9]\ttrain-error:0.073733\teval-error:0.1172\n",
      "[10]\ttrain-error:0.068667\teval-error:0.116\n",
      "[11]\ttrain-error:0.064933\teval-error:0.1164\n",
      "[12]\ttrain-error:0.062267\teval-error:0.1112\n",
      "[13]\ttrain-error:0.060533\teval-error:0.1116\n",
      "[14]\ttrain-error:0.0568\teval-error:0.1112\n",
      "[15]\ttrain-error:0.0504\teval-error:0.1048\n",
      "[16]\ttrain-error:0.0492\teval-error:0.1004\n",
      "[17]\ttrain-error:0.0464\teval-error:0.1016\n",
      "[18]\ttrain-error:0.044267\teval-error:0.1028\n",
      "[19]\ttrain-error:0.043467\teval-error:0.1012\n",
      "[20]\ttrain-error:0.038667\teval-error:0.1016\n",
      "[21]\ttrain-error:0.036533\teval-error:0.1004\n",
      "[22]\ttrain-error:0.034933\teval-error:0.0996\n",
      "[23]\ttrain-error:0.033733\teval-error:0.1004\n",
      "[24]\ttrain-error:0.032533\teval-error:0.104\n",
      "[25]\ttrain-error:0.0312\teval-error:0.1032\n",
      "[26]\ttrain-error:0.0288\teval-error:0.102\n",
      "[27]\ttrain-error:0.027733\teval-error:0.1016\n",
      "[28]\ttrain-error:0.025733\teval-error:0.1004\n",
      "[29]\ttrain-error:0.0232\teval-error:0.1008\n",
      "[30]\ttrain-error:0.022933\teval-error:0.1024\n",
      "[31]\ttrain-error:0.020667\teval-error:0.0992\n",
      "[32]\ttrain-error:0.019867\teval-error:0.0992\n",
      "[33]\ttrain-error:0.019733\teval-error:0.0984\n",
      "[34]\ttrain-error:0.016933\teval-error:0.1028\n",
      "[35]\ttrain-error:0.017333\teval-error:0.1008\n",
      "[36]\ttrain-error:0.016267\teval-error:0.0988\n",
      "[37]\ttrain-error:0.0152\teval-error:0.0972\n",
      "[38]\ttrain-error:0.0148\teval-error:0.098\n",
      "[39]\ttrain-error:0.014533\teval-error:0.0972\n",
      "[40]\ttrain-error:0.013333\teval-error:0.0976\n",
      "[41]\ttrain-error:0.012533\teval-error:0.0948\n",
      "[42]\ttrain-error:0.012533\teval-error:0.0956\n",
      "[43]\ttrain-error:0.011467\teval-error:0.0948\n",
      "[44]\ttrain-error:0.0108\teval-error:0.0968\n",
      "[45]\ttrain-error:0.010933\teval-error:0.096\n",
      "[46]\ttrain-error:0.01\teval-error:0.0932\n",
      "[47]\ttrain-error:0.010267\teval-error:0.0924\n",
      "[48]\ttrain-error:0.0092\teval-error:0.0912\n",
      "[49]\ttrain-error:0.009467\teval-error:0.0912\n",
      "logloss: 0.2223\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# 特徴量と目的変数をxgboostのデータ構造に変換する\n",
    "dtrain = xgb.DMatrix(tr_x, label=tr_y)\n",
    "dvalid = xgb.DMatrix(va_x, label=va_y)\n",
    "dtest = xgb.DMatrix(test_x)\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "params = {'objective': 'binary:logistic', 'silent': 1, 'random_state': 71}\n",
    "num_round = 50\n",
    "\n",
    "# 学習の実行\n",
    "# バリデーションデータもモデルに渡し、学習の進行とともにスコアがどう変わるかモニタリングする\n",
    "# watchlistには学習データおよびバリデーションデータをセットする\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "model = xgb.train(params, dtrain, num_round, evals=watchlist)\n",
    "\n",
    "# バリデーションデータでのスコアの確認\n",
    "va_pred = model.predict(dvalid)\n",
    "score = log_loss(va_y, va_pred)\n",
    "print(f'logloss: {score:.4f}')\n",
    "\n",
    "# 予測（二値の予測値ではなく、1である確率を出力するようにしている）\n",
    "pred = model.predict(dtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.5 xgboost의 사용방법의 포인트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# 学習データとバリデーションデータのスコアのモニタリング\n",
    "# -----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.540881\teval-logloss:0.550034\n",
      "Multiple eval metrics have been passed: 'eval-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-logloss hasn't improved in 20 rounds.\n",
      "[1]\ttrain-logloss:0.452691\teval-logloss:0.47182\n",
      "[2]\ttrain-logloss:0.394817\teval-logloss:0.42026\n",
      "[3]\ttrain-logloss:0.351976\teval-logloss:0.385202\n",
      "[4]\ttrain-logloss:0.320213\teval-logloss:0.361498\n",
      "[5]\ttrain-logloss:0.296733\teval-logloss:0.344634\n",
      "[6]\ttrain-logloss:0.276105\teval-logloss:0.329003\n",
      "[7]\ttrain-logloss:0.258857\teval-logloss:0.316697\n",
      "[8]\ttrain-logloss:0.243628\teval-logloss:0.30775\n",
      "[9]\ttrain-logloss:0.231528\teval-logloss:0.300925\n",
      "[10]\ttrain-logloss:0.220163\teval-logloss:0.294131\n",
      "[11]\ttrain-logloss:0.209625\teval-logloss:0.285281\n",
      "[12]\ttrain-logloss:0.199507\teval-logloss:0.279123\n",
      "[13]\ttrain-logloss:0.193238\teval-logloss:0.276415\n",
      "[14]\ttrain-logloss:0.185473\teval-logloss:0.271543\n",
      "[15]\ttrain-logloss:0.174737\teval-logloss:0.265163\n",
      "[16]\ttrain-logloss:0.168997\teval-logloss:0.260891\n",
      "[17]\ttrain-logloss:0.163227\teval-logloss:0.25849\n",
      "[18]\ttrain-logloss:0.159501\teval-logloss:0.256912\n",
      "[19]\ttrain-logloss:0.156374\teval-logloss:0.255114\n",
      "[20]\ttrain-logloss:0.147222\teval-logloss:0.250345\n",
      "[21]\ttrain-logloss:0.142902\teval-logloss:0.247341\n",
      "[22]\ttrain-logloss:0.137821\teval-logloss:0.246117\n",
      "[23]\ttrain-logloss:0.133619\teval-logloss:0.243874\n",
      "[24]\ttrain-logloss:0.130467\teval-logloss:0.242507\n",
      "[25]\ttrain-logloss:0.126539\teval-logloss:0.240938\n",
      "[26]\ttrain-logloss:0.122679\teval-logloss:0.240048\n",
      "[27]\ttrain-logloss:0.119661\teval-logloss:0.238031\n",
      "[28]\ttrain-logloss:0.115058\teval-logloss:0.236994\n",
      "[29]\ttrain-logloss:0.110268\teval-logloss:0.236264\n",
      "[30]\ttrain-logloss:0.108272\teval-logloss:0.236213\n",
      "[31]\ttrain-logloss:0.102622\teval-logloss:0.232686\n",
      "[32]\ttrain-logloss:0.100615\teval-logloss:0.23212\n",
      "[33]\ttrain-logloss:0.09913\teval-logloss:0.231798\n",
      "[34]\ttrain-logloss:0.095822\teval-logloss:0.23184\n",
      "[35]\ttrain-logloss:0.093781\teval-logloss:0.229978\n",
      "[36]\ttrain-logloss:0.092428\teval-logloss:0.229803\n",
      "[37]\ttrain-logloss:0.089519\teval-logloss:0.229134\n",
      "[38]\ttrain-logloss:0.087322\teval-logloss:0.2287\n",
      "[39]\ttrain-logloss:0.085759\teval-logloss:0.227859\n",
      "[40]\ttrain-logloss:0.083396\teval-logloss:0.22857\n",
      "[41]\ttrain-logloss:0.081252\teval-logloss:0.226949\n",
      "[42]\ttrain-logloss:0.080268\teval-logloss:0.226455\n",
      "[43]\ttrain-logloss:0.078286\teval-logloss:0.226595\n",
      "[44]\ttrain-logloss:0.076156\teval-logloss:0.226067\n",
      "[45]\ttrain-logloss:0.075219\teval-logloss:0.224988\n",
      "[46]\ttrain-logloss:0.073133\teval-logloss:0.22316\n",
      "[47]\ttrain-logloss:0.071982\teval-logloss:0.222926\n",
      "[48]\ttrain-logloss:0.070255\teval-logloss:0.222653\n",
      "[49]\ttrain-logloss:0.069475\teval-logloss:0.222264\n",
      "[50]\ttrain-logloss:0.067254\teval-logloss:0.222274\n",
      "[51]\ttrain-logloss:0.066076\teval-logloss:0.221892\n",
      "[52]\ttrain-logloss:0.064741\teval-logloss:0.222578\n",
      "[53]\ttrain-logloss:0.06343\teval-logloss:0.222785\n",
      "[54]\ttrain-logloss:0.06259\teval-logloss:0.222803\n",
      "[55]\ttrain-logloss:0.061631\teval-logloss:0.222622\n",
      "[56]\ttrain-logloss:0.060562\teval-logloss:0.22149\n",
      "[57]\ttrain-logloss:0.058593\teval-logloss:0.22114\n",
      "[58]\ttrain-logloss:0.057962\teval-logloss:0.220926\n",
      "[59]\ttrain-logloss:0.056918\teval-logloss:0.219828\n",
      "[60]\ttrain-logloss:0.055642\teval-logloss:0.220266\n",
      "[61]\ttrain-logloss:0.055001\teval-logloss:0.220758\n",
      "[62]\ttrain-logloss:0.053926\teval-logloss:0.220377\n",
      "[63]\ttrain-logloss:0.053389\teval-logloss:0.22014\n",
      "[64]\ttrain-logloss:0.052521\teval-logloss:0.219513\n",
      "[65]\ttrain-logloss:0.050961\teval-logloss:0.217595\n",
      "[66]\ttrain-logloss:0.050045\teval-logloss:0.217166\n",
      "[67]\ttrain-logloss:0.049089\teval-logloss:0.216988\n",
      "[68]\ttrain-logloss:0.048196\teval-logloss:0.216233\n",
      "[69]\ttrain-logloss:0.047253\teval-logloss:0.215409\n",
      "[70]\ttrain-logloss:0.04671\teval-logloss:0.215588\n",
      "[71]\ttrain-logloss:0.045747\teval-logloss:0.215411\n",
      "[72]\ttrain-logloss:0.044629\teval-logloss:0.214068\n",
      "[73]\ttrain-logloss:0.044048\teval-logloss:0.213968\n",
      "[74]\ttrain-logloss:0.043008\teval-logloss:0.214959\n",
      "[75]\ttrain-logloss:0.042618\teval-logloss:0.215537\n",
      "[76]\ttrain-logloss:0.042185\teval-logloss:0.21631\n",
      "[77]\ttrain-logloss:0.041566\teval-logloss:0.215793\n",
      "[78]\ttrain-logloss:0.040765\teval-logloss:0.215914\n",
      "[79]\ttrain-logloss:0.040025\teval-logloss:0.216056\n",
      "[80]\ttrain-logloss:0.039717\teval-logloss:0.216423\n",
      "[81]\ttrain-logloss:0.038736\teval-logloss:0.215281\n",
      "[82]\ttrain-logloss:0.03837\teval-logloss:0.216057\n",
      "[83]\ttrain-logloss:0.038065\teval-logloss:0.215927\n",
      "[84]\ttrain-logloss:0.037374\teval-logloss:0.216652\n",
      "[85]\ttrain-logloss:0.036687\teval-logloss:0.217843\n",
      "[86]\ttrain-logloss:0.035757\teval-logloss:0.219184\n",
      "[87]\ttrain-logloss:0.035533\teval-logloss:0.219122\n",
      "[88]\ttrain-logloss:0.034801\teval-logloss:0.219667\n",
      "[89]\ttrain-logloss:0.034249\teval-logloss:0.21976\n",
      "[90]\ttrain-logloss:0.034018\teval-logloss:0.219961\n",
      "[91]\ttrain-logloss:0.033418\teval-logloss:0.219719\n",
      "[92]\ttrain-logloss:0.032483\teval-logloss:0.21991\n",
      "[93]\ttrain-logloss:0.031948\teval-logloss:0.219616\n",
      "Stopping. Best iteration:\n",
      "[73]\ttrain-logloss:0.044048\teval-logloss:0.213968\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# モニタリングをloglossで行い、アーリーストッピングの観察するroundを20とする\n",
    "params = {'objective': 'binary:logistic', 'silent': 1, 'random_state': 71,\n",
    "          'eval_metric': 'logloss'}\n",
    "num_round = 500\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "model = xgb.train(params, dtrain, num_round, evals=watchlist,\n",
    "                  early_stopping_rounds=20)\n",
    "\n",
    "# 最適な決定木の本数で予測を行う\n",
    "pred = model.predict(dtest, ntree_limit=model.best_ntree_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.6 lightgbm 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# lightgbmの実装\n",
    "# -----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\front\\anaconda3\\envs\\kaggle\\lib\\site-packages\\lightgbm\\basic.py:1295: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['medical_info_b2', 'medical_info_b3', 'product']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttrain's binary_logloss: 0.454286\tvalid's binary_logloss: 0.4654\n",
      "[2]\ttrain's binary_logloss: 0.429417\tvalid's binary_logloss: 0.443487\n",
      "[3]\ttrain's binary_logloss: 0.410142\tvalid's binary_logloss: 0.426359\n",
      "[4]\ttrain's binary_logloss: 0.393494\tvalid's binary_logloss: 0.411015\n",
      "[5]\ttrain's binary_logloss: 0.379488\tvalid's binary_logloss: 0.398589\n",
      "[6]\ttrain's binary_logloss: 0.366857\tvalid's binary_logloss: 0.386944\n",
      "[7]\ttrain's binary_logloss: 0.354417\tvalid's binary_logloss: 0.376575\n",
      "[8]\ttrain's binary_logloss: 0.34379\tvalid's binary_logloss: 0.367472\n",
      "[9]\ttrain's binary_logloss: 0.334998\tvalid's binary_logloss: 0.359954\n",
      "[10]\ttrain's binary_logloss: 0.325439\tvalid's binary_logloss: 0.35231\n",
      "[11]\ttrain's binary_logloss: 0.316396\tvalid's binary_logloss: 0.345017\n",
      "[12]\ttrain's binary_logloss: 0.309224\tvalid's binary_logloss: 0.340222\n",
      "[13]\ttrain's binary_logloss: 0.301732\tvalid's binary_logloss: 0.333364\n",
      "[14]\ttrain's binary_logloss: 0.294708\tvalid's binary_logloss: 0.328792\n",
      "[15]\ttrain's binary_logloss: 0.288201\tvalid's binary_logloss: 0.324163\n",
      "[16]\ttrain's binary_logloss: 0.282431\tvalid's binary_logloss: 0.319925\n",
      "[17]\ttrain's binary_logloss: 0.276254\tvalid's binary_logloss: 0.315333\n",
      "[18]\ttrain's binary_logloss: 0.271291\tvalid's binary_logloss: 0.312272\n",
      "[19]\ttrain's binary_logloss: 0.265568\tvalid's binary_logloss: 0.308592\n",
      "[20]\ttrain's binary_logloss: 0.260806\tvalid's binary_logloss: 0.305423\n",
      "[21]\ttrain's binary_logloss: 0.256176\tvalid's binary_logloss: 0.302695\n",
      "[22]\ttrain's binary_logloss: 0.251765\tvalid's binary_logloss: 0.300585\n",
      "[23]\ttrain's binary_logloss: 0.247005\tvalid's binary_logloss: 0.29676\n",
      "[24]\ttrain's binary_logloss: 0.242821\tvalid's binary_logloss: 0.294192\n",
      "[25]\ttrain's binary_logloss: 0.23893\tvalid's binary_logloss: 0.291965\n",
      "[26]\ttrain's binary_logloss: 0.234672\tvalid's binary_logloss: 0.288845\n",
      "[27]\ttrain's binary_logloss: 0.230955\tvalid's binary_logloss: 0.286873\n",
      "[28]\ttrain's binary_logloss: 0.227917\tvalid's binary_logloss: 0.284952\n",
      "[29]\ttrain's binary_logloss: 0.22419\tvalid's binary_logloss: 0.282441\n",
      "[30]\ttrain's binary_logloss: 0.220358\tvalid's binary_logloss: 0.279985\n",
      "[31]\ttrain's binary_logloss: 0.216471\tvalid's binary_logloss: 0.276463\n",
      "[32]\ttrain's binary_logloss: 0.213126\tvalid's binary_logloss: 0.274941\n",
      "[33]\ttrain's binary_logloss: 0.209834\tvalid's binary_logloss: 0.27273\n",
      "[34]\ttrain's binary_logloss: 0.206753\tvalid's binary_logloss: 0.271164\n",
      "[35]\ttrain's binary_logloss: 0.204203\tvalid's binary_logloss: 0.269744\n",
      "[36]\ttrain's binary_logloss: 0.200539\tvalid's binary_logloss: 0.266825\n",
      "[37]\ttrain's binary_logloss: 0.197168\tvalid's binary_logloss: 0.265267\n",
      "[38]\ttrain's binary_logloss: 0.194498\tvalid's binary_logloss: 0.263992\n",
      "[39]\ttrain's binary_logloss: 0.192032\tvalid's binary_logloss: 0.263437\n",
      "[40]\ttrain's binary_logloss: 0.189326\tvalid's binary_logloss: 0.261947\n",
      "[41]\ttrain's binary_logloss: 0.186858\tvalid's binary_logloss: 0.260294\n",
      "[42]\ttrain's binary_logloss: 0.18455\tvalid's binary_logloss: 0.259234\n",
      "[43]\ttrain's binary_logloss: 0.181977\tvalid's binary_logloss: 0.257863\n",
      "[44]\ttrain's binary_logloss: 0.179512\tvalid's binary_logloss: 0.256472\n",
      "[45]\ttrain's binary_logloss: 0.177167\tvalid's binary_logloss: 0.255215\n",
      "[46]\ttrain's binary_logloss: 0.174983\tvalid's binary_logloss: 0.254662\n",
      "[47]\ttrain's binary_logloss: 0.172989\tvalid's binary_logloss: 0.254382\n",
      "[48]\ttrain's binary_logloss: 0.170821\tvalid's binary_logloss: 0.253217\n",
      "[49]\ttrain's binary_logloss: 0.168089\tvalid's binary_logloss: 0.251035\n",
      "[50]\ttrain's binary_logloss: 0.16612\tvalid's binary_logloss: 0.250534\n",
      "[51]\ttrain's binary_logloss: 0.164304\tvalid's binary_logloss: 0.249786\n",
      "[52]\ttrain's binary_logloss: 0.162127\tvalid's binary_logloss: 0.24821\n",
      "[53]\ttrain's binary_logloss: 0.160416\tvalid's binary_logloss: 0.247239\n",
      "[54]\ttrain's binary_logloss: 0.158818\tvalid's binary_logloss: 0.246814\n",
      "[55]\ttrain's binary_logloss: 0.156032\tvalid's binary_logloss: 0.244454\n",
      "[56]\ttrain's binary_logloss: 0.154359\tvalid's binary_logloss: 0.243893\n",
      "[57]\ttrain's binary_logloss: 0.152642\tvalid's binary_logloss: 0.243225\n",
      "[58]\ttrain's binary_logloss: 0.151044\tvalid's binary_logloss: 0.242837\n",
      "[59]\ttrain's binary_logloss: 0.149127\tvalid's binary_logloss: 0.241454\n",
      "[60]\ttrain's binary_logloss: 0.146802\tvalid's binary_logloss: 0.239391\n",
      "[61]\ttrain's binary_logloss: 0.145309\tvalid's binary_logloss: 0.238612\n",
      "[62]\ttrain's binary_logloss: 0.143786\tvalid's binary_logloss: 0.238014\n",
      "[63]\ttrain's binary_logloss: 0.142085\tvalid's binary_logloss: 0.236823\n",
      "[64]\ttrain's binary_logloss: 0.140588\tvalid's binary_logloss: 0.236355\n",
      "[65]\ttrain's binary_logloss: 0.138946\tvalid's binary_logloss: 0.235721\n",
      "[66]\ttrain's binary_logloss: 0.137666\tvalid's binary_logloss: 0.235887\n",
      "[67]\ttrain's binary_logloss: 0.135987\tvalid's binary_logloss: 0.235729\n",
      "[68]\ttrain's binary_logloss: 0.134551\tvalid's binary_logloss: 0.235073\n",
      "[69]\ttrain's binary_logloss: 0.133163\tvalid's binary_logloss: 0.234729\n",
      "[70]\ttrain's binary_logloss: 0.131297\tvalid's binary_logloss: 0.233257\n",
      "[71]\ttrain's binary_logloss: 0.12978\tvalid's binary_logloss: 0.232796\n",
      "[72]\ttrain's binary_logloss: 0.128516\tvalid's binary_logloss: 0.232675\n",
      "[73]\ttrain's binary_logloss: 0.12705\tvalid's binary_logloss: 0.232375\n",
      "[74]\ttrain's binary_logloss: 0.12569\tvalid's binary_logloss: 0.231574\n",
      "[75]\ttrain's binary_logloss: 0.124247\tvalid's binary_logloss: 0.230772\n",
      "[76]\ttrain's binary_logloss: 0.122886\tvalid's binary_logloss: 0.229912\n",
      "[77]\ttrain's binary_logloss: 0.121689\tvalid's binary_logloss: 0.229134\n",
      "[78]\ttrain's binary_logloss: 0.120185\tvalid's binary_logloss: 0.227908\n",
      "[79]\ttrain's binary_logloss: 0.119006\tvalid's binary_logloss: 0.227255\n",
      "[80]\ttrain's binary_logloss: 0.117758\tvalid's binary_logloss: 0.226919\n",
      "[81]\ttrain's binary_logloss: 0.116367\tvalid's binary_logloss: 0.226063\n",
      "[82]\ttrain's binary_logloss: 0.115242\tvalid's binary_logloss: 0.225734\n",
      "[83]\ttrain's binary_logloss: 0.114066\tvalid's binary_logloss: 0.225842\n",
      "[84]\ttrain's binary_logloss: 0.113021\tvalid's binary_logloss: 0.225589\n",
      "[85]\ttrain's binary_logloss: 0.111898\tvalid's binary_logloss: 0.22567\n",
      "[86]\ttrain's binary_logloss: 0.110745\tvalid's binary_logloss: 0.225063\n",
      "[87]\ttrain's binary_logloss: 0.109676\tvalid's binary_logloss: 0.225015\n",
      "[88]\ttrain's binary_logloss: 0.10853\tvalid's binary_logloss: 0.225252\n",
      "[89]\ttrain's binary_logloss: 0.107014\tvalid's binary_logloss: 0.223305\n",
      "[90]\ttrain's binary_logloss: 0.105649\tvalid's binary_logloss: 0.222349\n",
      "[91]\ttrain's binary_logloss: 0.10459\tvalid's binary_logloss: 0.221952\n",
      "[92]\ttrain's binary_logloss: 0.103518\tvalid's binary_logloss: 0.221524\n",
      "[93]\ttrain's binary_logloss: 0.102541\tvalid's binary_logloss: 0.221294\n",
      "[94]\ttrain's binary_logloss: 0.101574\tvalid's binary_logloss: 0.221408\n",
      "[95]\ttrain's binary_logloss: 0.100554\tvalid's binary_logloss: 0.221599\n",
      "[96]\ttrain's binary_logloss: 0.0994329\tvalid's binary_logloss: 0.221159\n",
      "[97]\ttrain's binary_logloss: 0.09839\tvalid's binary_logloss: 0.220885\n",
      "[98]\ttrain's binary_logloss: 0.0974307\tvalid's binary_logloss: 0.22092\n",
      "[99]\ttrain's binary_logloss: 0.0965374\tvalid's binary_logloss: 0.220945\n",
      "[100]\ttrain's binary_logloss: 0.0956876\tvalid's binary_logloss: 0.221081\n",
      "logloss: 0.2211\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# 特徴量と目的変数をlightgbmのデータ構造に変換する\n",
    "lgb_train = lgb.Dataset(tr_x, tr_y)\n",
    "lgb_eval = lgb.Dataset(va_x, va_y)\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "params = {'objective': 'binary', 'seed': 71, 'verbose': 0, 'metrics': 'binary_logloss'}\n",
    "num_round = 100\n",
    "\n",
    "# 学習の実行\n",
    "# カテゴリ変数をパラメータで指定している\n",
    "# バリデーションデータもモデルに渡し、学習の進行とともにスコアがどう変わるかモニタリングする\n",
    "categorical_features = ['product', 'medical_info_b2', 'medical_info_b3']\n",
    "model = lgb.train(params, lgb_train, num_boost_round=num_round,\n",
    "                  categorical_feature=categorical_features,\n",
    "                  valid_names=['train', 'valid'], valid_sets=[lgb_train, lgb_eval])\n",
    "\n",
    "# バリデーションデータでのスコアの確認\n",
    "va_pred = model.predict(va_x)\n",
    "score = log_loss(va_y, va_pred)\n",
    "print(f'logloss: {score:.4f}')\n",
    "\n",
    "# 予測\n",
    "pred = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 신경망"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------\n",
    "# データ等の準備\n",
    "# ----------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# train_xは学習データ、train_yは目的変数、test_xはテストデータ\n",
    "# pandasのDataFrame, Seriesで保持します。（numpyのarrayで保持することもあります）\n",
    "# one-hot encodingされたものを読み込む\n",
    "\n",
    "# train = pd.read_csv('../input/sample-data/train_preprocessed_onehot.csv')\n",
    "train = pd.read_csv('../input/sample-data/train_preprocessed_onehot_01.csv')  # 에러 해결\n",
    "train_x = train.drop(['target'], axis=1)\n",
    "train_y = train['target']\n",
    "# test_x = pd.read_csv('../input/sample-data/test_preprocessed_onehot.csv')\n",
    "test_x = pd.read_csv('../input/sample-data/test_preprocessed_onehot_01.csv')\n",
    "\n",
    "# 学習データを学習データとバリデーションデータに分ける\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=71)\n",
    "tr_idx, va_idx = list(kf.split(train_x))[0]\n",
    "tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n",
    "tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n",
    "\n",
    "# tensorflowの警告抑制\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# ニューラルネットの実装\n",
    "# -----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/10\n",
      "7500/7500 [==============================] - 2s 263us/step - loss: 0.4374 - accuracy: 0.8227 - val_loss: 0.3854 - val_accuracy: 0.8496\n",
      "Epoch 2/10\n",
      "7500/7500 [==============================] - 1s 188us/step - loss: 0.3639 - accuracy: 0.8540 - val_loss: 0.3801 - val_accuracy: 0.8464\n",
      "Epoch 3/10\n",
      "7500/7500 [==============================] - 1s 162us/step - loss: 0.3433 - accuracy: 0.8631 - val_loss: 0.3758 - val_accuracy: 0.8464\n",
      "Epoch 4/10\n",
      "7500/7500 [==============================] - 1s 146us/step - loss: 0.3243 - accuracy: 0.8688 - val_loss: 0.3668 - val_accuracy: 0.8476\n",
      "Epoch 5/10\n",
      "7500/7500 [==============================] - 1s 167us/step - loss: 0.3032 - accuracy: 0.8796 - val_loss: 0.3572 - val_accuracy: 0.8524\n",
      "Epoch 6/10\n",
      "7500/7500 [==============================] - 1s 159us/step - loss: 0.2835 - accuracy: 0.8843 - val_loss: 0.3394 - val_accuracy: 0.8616\n",
      "Epoch 7/10\n",
      "7500/7500 [==============================] - 1s 150us/step - loss: 0.2522 - accuracy: 0.8988 - val_loss: 0.3248 - val_accuracy: 0.8644\n",
      "Epoch 8/10\n",
      "7500/7500 [==============================] - 1s 108us/step - loss: 0.2216 - accuracy: 0.9128 - val_loss: 0.3081 - val_accuracy: 0.8708\n",
      "Epoch 9/10\n",
      "7500/7500 [==============================] - 1s 127us/step - loss: 0.2020 - accuracy: 0.9177 - val_loss: 0.2936 - val_accuracy: 0.8732\n",
      "Epoch 10/10\n",
      "7500/7500 [==============================] - 1s 95us/step - loss: 0.1767 - accuracy: 0.9305 - val_loss: 0.2934 - val_accuracy: 0.8776\n",
      "logloss: 0.2934\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# データのスケーリング\n",
    "scaler = StandardScaler()\n",
    "tr_x = scaler.fit_transform(tr_x)\n",
    "va_x = scaler.transform(va_x)\n",
    "test_x = scaler.transform(test_x)\n",
    "\n",
    "# ニューラルネットモデルの構築\n",
    "model = Sequential()\n",
    "model.add(Dense(256, activation='relu', input_shape=(train_x.shape[1],)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 学習の実行\n",
    "# バリデーションデータもモデルに渡し、学習の進行とともにスコアがどう変わるかモニタリングする\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "history = model.fit(tr_x, tr_y,\n",
    "                    batch_size=batch_size, epochs=epochs,\n",
    "                    verbose=1, validation_data=(va_x, va_y))\n",
    "\n",
    "# バリデーションデータでのスコアの確認\n",
    "va_pred = model.predict(va_x)\n",
    "score = log_loss(va_y, va_pred, eps=1e-7)\n",
    "print(f'logloss: {score:.4f}')\n",
    "\n",
    "# 予測\n",
    "pred = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
