{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cff07ebc",
   "metadata": {},
   "source": [
    "### 데이터 분할 방법과 모델 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6ad993",
   "metadata": {},
   "source": [
    "### 학습 내용\n",
    " * 홀드아웃(Hold-out) 방법\n",
    " * 교차 검증(Cross-validation)\n",
    " * Stratified K-Fold\n",
    " * GroupKFold\n",
    " * Leave-one-out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b20454",
   "metadata": {},
   "source": [
    "### 데이터 및 라이브러리 사전 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e7a1661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# train_x는 학습 데이터, train_y는 목적 변수, test_x는 테스트 데이터\n",
    "# pandas의 DataFrame, Series로 유지합니다.(numpy의 array로 유지하기도 합니다)\n",
    "\n",
    "train = pd.read_csv('../input/sample-data/train_preprocessed.csv')\n",
    "train_x = train.drop(['target'], axis=1)\n",
    "train_y = train['target']\n",
    "test_x = pd.read_csv('../input/sample-data/test_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67295b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 28), (10000,), (10000, 28))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape, train_y.shape, test_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9c41a4",
   "metadata": {},
   "source": [
    "### XGBoost를 활용한 Model 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d85d273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost에 의한 학습·예측을 하는 클래스\n",
    "import xgboost as xgb\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, params=None):\n",
    "        self.model = None\n",
    "        if params is None:\n",
    "            self.params = {}\n",
    "        else:\n",
    "            self.params = params\n",
    "\n",
    "    def fit(self, tr_x, tr_y, va_x, va_y):\n",
    "        # params = {'objective': 'binary:logistic', 'silent': 1, 'random_state': 71}\n",
    "        params = {'objective': 'binary:logistic', 'verbosity': 0, 'random_state': 71}\n",
    "        params.update(self.params)\n",
    "        num_round = 10\n",
    "        dtrain = xgb.DMatrix(tr_x, label=tr_y)\n",
    "        dvalid = xgb.DMatrix(va_x, label=va_y)\n",
    "        watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "        self.model = xgb.train(params, dtrain, num_round, evals=watchlist)\n",
    "\n",
    "    def predict(self, x):\n",
    "        data = xgb.DMatrix(x)\n",
    "        pred = self.model.predict(data)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6d111c",
   "metadata": {},
   "source": [
    "### 홀드아웃(Hold-out) 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b237328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.54088\teval-logloss:0.55003\n",
      "[1]\ttrain-logloss:0.45269\teval-logloss:0.47182\n",
      "[2]\ttrain-logloss:0.39482\teval-logloss:0.42026\n",
      "[3]\ttrain-logloss:0.35198\teval-logloss:0.38520\n",
      "[4]\ttrain-logloss:0.32021\teval-logloss:0.36150\n",
      "[5]\ttrain-logloss:0.29673\teval-logloss:0.34463\n",
      "[6]\ttrain-logloss:0.27610\teval-logloss:0.32900\n",
      "[7]\ttrain-logloss:0.25886\teval-logloss:0.31670\n",
      "[8]\ttrain-logloss:0.24363\teval-logloss:0.30775\n",
      "[9]\ttrain-logloss:0.23153\teval-logloss:0.30093\n",
      "0.30092523164749146\n",
      "[   0    1    2 ... 9996 9998 9999] [   6   10   11 ... 9994 9995 9997]\n"
     ]
    }
   ],
   "source": [
    "# 홀드아웃(hold-out)방법으로 검증 데이터의 분할\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train_test_split()함수를 이용한 홀드아웃 방법으로 분할\n",
    "tr_x, va_x, tr_y, va_y = train_test_split(train_x, train_y,\n",
    "                                          test_size=0.25, random_state=71, shuffle=True)\n",
    "\n",
    "# -----------------------------------\n",
    "# 홀드아웃(hold-out)방법으로 검증을 수행\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Model 클래스를 정의\n",
    "# Model 클래스는 fit으로 학습하고 predict로 예측값 확률을 출력\n",
    "\n",
    "# train_test_split 함수를 이용하여 홀드아웃 방법으로 분할\n",
    "tr_x, va_x, tr_y, va_y = train_test_split(train_x, train_y,\n",
    "                                          test_size=0.25, random_state=71, shuffle=True)\n",
    "\n",
    "# 학습 실행, 검증 데이터 예측값 출력, 점수 계산\n",
    "model = Model()\n",
    "model.fit(tr_x, tr_y, va_x, va_y)\n",
    "va_pred = model.predict(va_x)\n",
    "score = log_loss(va_y, va_pred)\n",
    "print(score)\n",
    "\n",
    "# -----------------------------------\n",
    "# KFold 클래스를 이용하여 홀드아웃 방법으로 검증 데이터를 분할\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# KFold 클래스를 이용하여 홀드아웃 방법으로 분할\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=71)\n",
    "tr_idx, va_idx = list(kf.split(train_x))[0]\n",
    "print(tr_idx, va_idx)\n",
    "\n",
    "tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n",
    "tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd9091a",
   "metadata": {},
   "source": [
    "### 교차 검증 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95aac71d",
   "metadata": {},
   "source": [
    "* 교차 검증은 주어진 데이터를 여러 개의 폴드로 나누어 모델을 여러 번 학습하고 평가하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4e5f15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.54088\teval-logloss:0.55003\n",
      "[1]\ttrain-logloss:0.45269\teval-logloss:0.47182\n",
      "[2]\ttrain-logloss:0.39482\teval-logloss:0.42026\n",
      "[3]\ttrain-logloss:0.35198\teval-logloss:0.38520\n",
      "[4]\ttrain-logloss:0.32021\teval-logloss:0.36150\n",
      "[5]\ttrain-logloss:0.29673\teval-logloss:0.34463\n",
      "[6]\ttrain-logloss:0.27610\teval-logloss:0.32900\n",
      "[7]\ttrain-logloss:0.25886\teval-logloss:0.31670\n",
      "[8]\ttrain-logloss:0.24363\teval-logloss:0.30775\n",
      "[9]\ttrain-logloss:0.23153\teval-logloss:0.30093\n",
      "[0]\ttrain-logloss:0.53891\teval-logloss:0.54864\n",
      "[1]\ttrain-logloss:0.45219\teval-logloss:0.47149\n",
      "[2]\ttrain-logloss:0.39574\teval-logloss:0.41998\n",
      "[3]\ttrain-logloss:0.35476\teval-logloss:0.38413\n",
      "[4]\ttrain-logloss:0.32218\teval-logloss:0.35626\n",
      "[5]\ttrain-logloss:0.29945\teval-logloss:0.33910\n",
      "[6]\ttrain-logloss:0.27783\teval-logloss:0.32552\n",
      "[7]\ttrain-logloss:0.26326\teval-logloss:0.31573\n",
      "[8]\ttrain-logloss:0.24780\teval-logloss:0.30592\n",
      "[9]\ttrain-logloss:0.23369\teval-logloss:0.29596\n",
      "[0]\ttrain-logloss:0.54332\teval-logloss:0.55058\n",
      "[1]\ttrain-logloss:0.45437\teval-logloss:0.46830\n",
      "[2]\ttrain-logloss:0.39712\teval-logloss:0.41763\n",
      "[3]\ttrain-logloss:0.35413\teval-logloss:0.38086\n",
      "[4]\ttrain-logloss:0.32187\teval-logloss:0.35824\n",
      "[5]\ttrain-logloss:0.29769\teval-logloss:0.33834\n",
      "[6]\ttrain-logloss:0.27822\teval-logloss:0.32579\n",
      "[7]\ttrain-logloss:0.26050\teval-logloss:0.31308\n",
      "[8]\ttrain-logloss:0.24437\teval-logloss:0.30016\n",
      "[9]\ttrain-logloss:0.23099\teval-logloss:0.29331\n",
      "[0]\ttrain-logloss:0.54166\teval-logloss:0.55012\n",
      "[1]\ttrain-logloss:0.45309\teval-logloss:0.46965\n",
      "[2]\ttrain-logloss:0.39439\teval-logloss:0.41932\n",
      "[3]\ttrain-logloss:0.35366\teval-logloss:0.38286\n",
      "[4]\ttrain-logloss:0.31902\teval-logloss:0.35792\n",
      "[5]\ttrain-logloss:0.29187\teval-logloss:0.33824\n",
      "[6]\ttrain-logloss:0.27289\teval-logloss:0.32628\n",
      "[7]\ttrain-logloss:0.25669\teval-logloss:0.31550\n",
      "[8]\ttrain-logloss:0.23894\teval-logloss:0.30375\n",
      "[9]\ttrain-logloss:0.22701\teval-logloss:0.29646\n",
      "0.29666262172833086\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# 교차 검증\n",
    "# -----------------------------------\n",
    "# 교차 검증 방법으로 데이터 분할\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# KFold 클래스를 이용하여 교차 검증 분할을 수행\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=71)\n",
    "for tr_idx, va_idx in kf.split(train_x):\n",
    "    tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n",
    "    tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n",
    "\n",
    "# -----------------------------------\n",
    "# 교차 검증을 수행\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Model 클래스를 정의\n",
    "# Model 클래스는 fit으로 학습하고, predict로 예측값 확률을 출력\n",
    "\n",
    "scores = []\n",
    "\n",
    "# KFold 클래스를 이용하여 교차 검증 방법으로 분할\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=71)\n",
    "for tr_idx, va_idx in kf.split(train_x):\n",
    "    tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n",
    "    tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n",
    "\n",
    "    # 학습 실행, 검증 데이터의 예측값 출력, 점수 계산\n",
    "    model = Model()\n",
    "    model.fit(tr_x, tr_y, va_x, va_y)\n",
    "    va_pred = model.predict(va_x)\n",
    "    score = log_loss(va_y, va_pred)\n",
    "    scores.append(score)\n",
    "\n",
    "# 각 폴더의 점수 평균을 출력\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e2cb2d",
   "metadata": {},
   "source": [
    "### Stratified K-Fold 데이터 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "886353d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "특징(X) 학습, 평가 :  (7500, 28) (2500, 28)\n",
      "target(y) 학습, 평가 :  (7500,) (2500,)\n",
      "특징(X) 학습, 평가 :  (7500, 28) (2500, 28)\n",
      "target(y) 학습, 평가 :  (7500,) (2500,)\n",
      "특징(X) 학습, 평가 :  (7500, 28) (2500, 28)\n",
      "target(y) 학습, 평가 :  (7500,) (2500,)\n",
      "특징(X) 학습, 평가 :  (7500, 28) (2500, 28)\n",
      "target(y) 학습, 평가 :  (7500,) (2500,)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# Stratified K-Fold\n",
    "# -----------------------------------\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# StratifiedKFold 클래스를 이용하여 층화추출로 데이터 분할\n",
    "kf = StratifiedKFold(n_splits=4, shuffle=True, random_state=71)\n",
    "for tr_idx, va_idx in kf.split(train_x, train_y):\n",
    "    tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n",
    "    tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n",
    "    \n",
    "    # 23/06 추가\n",
    "    print(\"특징(X) 학습, 평가 : \", tr_x.shape, va_x.shape)\n",
    "    print(\"target(y) 학습, 평가 : \", tr_y.shape, va_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11fa93a",
   "metadata": {},
   "source": [
    "### GroupKFold 데이터 분할"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a50b1ee",
   "metadata": {},
   "source": [
    "* GroupKFold는 데이터셋을 고유한 그룹으로 분할하는 교차 검증 방법입니다. 예를 들어, 시계열 데이터나 고객별 데이터에서 각 고객이 독립적인 그룹을 형성할 때 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2ff30cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# GroupKFold\n",
    "# -----------------------------------\n",
    "# 4건씩 같은 유저가 있는 데이터였다고 가정한다.\n",
    "train_x['user_id'] = np.arange(0, len(train_x)) // 4\n",
    "# -----------------------------------\n",
    "\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "\n",
    "# user_id열의 고객 ID 단위로 분할\n",
    "user_id = train_x['user_id']\n",
    "unique_user_ids = user_id.unique()\n",
    "\n",
    "# KFold 클래스를 이용하여 고객 ID 단위로 분할\n",
    "scores = []\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=71)\n",
    "for tr_group_idx, va_group_idx in kf.split(unique_user_ids):\n",
    "    # 고객 ID를 train/valid(학습에 사용하는 데이터, 검증 데이터)로 분할\n",
    "    tr_groups, va_groups = unique_user_ids[tr_group_idx], unique_user_ids[va_group_idx]\n",
    "\n",
    "    # 각 샘플의 고객 ID가 train/valid 중 어느 쪽에 속해 있느냐에 따라 분할\n",
    "    is_tr = user_id.isin(tr_groups)\n",
    "    is_va = user_id.isin(va_groups)\n",
    "    tr_x, va_x = train_x[is_tr], train_x[is_va]\n",
    "    tr_y, va_y = train_y[is_tr], train_y[is_va]\n",
    "\n",
    "# (참고)GroupKFold 클래스에서는 셔플과 난수 시드를 지정할 수 없으므로 사용하기 어려움\n",
    "kf = GroupKFold(n_splits=4)\n",
    "for tr_idx, va_idx in kf.split(train_x, train_y, user_id):\n",
    "    tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n",
    "    tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba378d0",
   "metadata": {},
   "source": [
    "### leave-one-out 데이터 분할"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f81b72",
   "metadata": {},
   "source": [
    "* Leave-One-Out 교차 검증은 각 데이터 포인트를 검증 데이터로 사용하고 나머지 데이터를 학습 데이터로 사용하여 모델을 평가하는 방법입니다. 데이터셋의 크기가 작을 때 유용하며, 모든 데이터를 한 번씩 검증에 활용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "691dded4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분할 index (학습, 평가) [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99] [19]\n",
      "학습(X), 평가(X) :  (99, 29) (1, 29)\n",
      "학습(y), 평가(y) :  (99,) (1,)\n",
      "분할 index (학습, 평가) [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99] [39]\n",
      "학습(X), 평가(X) :  (99, 29) (1, 29)\n",
      "학습(y), 평가(y) :  (99,) (1,)\n",
      "분할 index (학습, 평가) [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99] [59]\n",
      "학습(X), 평가(X) :  (99, 29) (1, 29)\n",
      "학습(y), 평가(y) :  (99,) (1,)\n",
      "분할 index (학습, 평가) [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99] [79]\n",
      "학습(X), 평가(X) :  (99, 29) (1, 29)\n",
      "학습(y), 평가(y) :  (99,) (1,)\n",
      "분할 index (학습, 평가) [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98] [99]\n",
      "학습(X), 평가(X) :  (99, 29) (1, 29)\n",
      "학습(y), 평가(y) :  (99,) (1,)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# leave-one-out\n",
    "# -----------------------------------\n",
    "# 데이터가 100건밖에 없는 것으로 간주\n",
    "train_x = train_x.iloc[:100, :].copy()\n",
    "# -----------------------------------\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "# 23/06 추가 - for문 20의 배수로 출력하여 확인\n",
    "cnt = 0\n",
    "for tr_idx, va_idx in loo.split(train_x):\n",
    "    tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n",
    "    tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n",
    "\n",
    "    # 23/06 추가\n",
    "    cnt +=1\n",
    "    if cnt%20==0:\n",
    "        print(\"분할 index (학습, 평가)\", tr_idx, va_idx)\n",
    "        print(\"학습(X), 평가(X) : \", tr_x.shape, va_x.shape)\n",
    "        print(\"학습(y), 평가(y) : \", tr_y.shape, va_y.shape) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
