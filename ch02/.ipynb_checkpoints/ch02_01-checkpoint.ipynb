{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평가지표 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 내용\n",
    " * 회귀의 평가지표 RMSE에 대해 실습을 통해 알아봅니다.\n",
    " * 분류의 다양한 평가지표에 대해 실습을 통해 알아봅니다. \n",
    "   * 혼동 행렬, F1-Score, log-loss, MAP@K 등"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평가지표란 무엇인가요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 평가지표(Evaluation metric)는 모델의 성능을 측정하기 위해 사용되는 지표.\n",
    "* 보통 모델이 예측한 결과와 실제값 사이의 차이를 측정하여 모델의 예측능력을 평가한다.\n",
    "* 머신러닝 과제에 분류, 회귀에 따라 사용되는 평가지표가 조금씩 다릅니다.\n",
    "* 회귀(Regression)의 문제에서는 아래와 같은 평가지표가 사용.\n",
    "   * 평균 제곱 오차 - MSE(Mean Squared Error, MSE)\n",
    "   * 평균 절대 오차 - MAE(Mean Absolute Error, MAE)\n",
    "   * 결정 계수(Coefficient of Determination, R-squared)\n",
    "   * 평균 제곱근 오차 - RMSE(Root Mean Squared Error) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 분류의 문제에 사용되는 기본 평가지표\n",
    "  * 정확도(accuracy)\n",
    "  * 정밀도(precision), 재현율(recall), F1-score, ROC곡선의 AUC(Area Under the Curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 회귀의 평가지표 - RMSE 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5531726674375732\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# 회귀\n",
    "# -----------------------------------\n",
    "# rmse\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# y_true - 실젯값, y_pred - 예측값\n",
    "y_true = [1.0, 1.5, 2.0, 1.2, 1.8]\n",
    "y_pred = [0.8, 1.5, 1.8, 1.3, 3.0]\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "print(rmse)\n",
    "# 0.5532"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분류의 평가지표 이해하기 - 혼동 행렬(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 1]\n",
      " [2 2]]\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# 이진 분류\n",
    "# -----------------------------------\n",
    "# 혼동행렬\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# 0, 1로 표현되는 이진 분류의 실젯값과 예측값\n",
    "y_true = [1, 0, 1, 1, 0, 1, 1, 0]\n",
    "y_pred = [0, 0, 1, 1, 0, 0, 1, 1]\n",
    "\n",
    "tp = np.sum((np.array(y_true) == 1) & (np.array(y_pred) == 1))\n",
    "tn = np.sum((np.array(y_true) == 0) & (np.array(y_pred) == 0))\n",
    "fp = np.sum((np.array(y_true) == 0) & (np.array(y_pred) == 1))\n",
    "fn = np.sum((np.array(y_true) == 1) & (np.array(y_pred) == 0))\n",
    "\n",
    "confusion_matrix1 = np.array([[tp, fp],\n",
    "                              [fn, tn]])\n",
    "\n",
    "print(confusion_matrix1)\n",
    "# array([[3, 1],\n",
    "#        [2, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1]\n",
      " [2 3]]\n"
     ]
    }
   ],
   "source": [
    "# 사이킷런의 metrics 모듈의 confusion_matrix로도 작성 가능하지만,\n",
    "# 혼동행렬의 요소 배치가 다르므로 주의가 필요\n",
    "confusion_matrix2 = confusion_matrix(y_true, y_pred)\n",
    "print(confusion_matrix2)\n",
    "# array([[2, 1],\n",
    "#        [2, 3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분류의 평가지표 - 정확도(Accuracy)\n",
    " * 정확도는 얼마나 정확하게 예측했는가를 나타내는 지표"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.625\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# 정확도(accuracy)\n",
    "# -----------------------------------\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 0, 1로 표현되는 이진 분류의 실젯값과 예측값\n",
    "y_true = [1, 0, 1, 1, 0, 1, 1, 0]\n",
    "y_pred = [0, 0, 1, 1, 0, 0, 1, 1]\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(accuracy)\n",
    "# 0.625"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분류 모델의 평가지표 - 로그 손실(Log Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logloss = (-1/n) * Σ[y * log(p) + (1 - y) * log(1 - p)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7135581778200728\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# logloss\n",
    "# -----------------------------------\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# 0, 1로 나타나는 이진 분류의 실젯값과 예측 확률\n",
    "y_true = [1, 0, 1, 1, 0, 1]\n",
    "y_prob = [0.1, 0.2, 0.8, 0.8, 0.1, 0.3]\n",
    "\n",
    "logloss = log_loss(y_true, y_prob)\n",
    "print(logloss)\n",
    "# 0.7136"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분류 모델의 평가지표 - 로그 손실(Log Loss)[다중 클래스 분류]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3625557672904274\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# 다중 클래스 분류\n",
    "# -----------------------------------\n",
    "# multi-class logloss\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# 3 클래스 분류의 실젯값과 예측값\n",
    "y_true = np.array([0, 2, 1, 2, 2])\n",
    "y_pred = np.array([[0.68, 0.32, 0.00],\n",
    "                   [0.00, 0.00, 1.00],\n",
    "                   [0.60, 0.40, 0.00],\n",
    "                   [0.00, 0.00, 1.00],\n",
    "                   [0.28, 0.12, 0.60]])\n",
    "logloss = log_loss(y_true, y_pred)\n",
    "print(logloss)\n",
    "# 0.3626"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분류 모델의 평가지표 - F1-score\n",
    " * 정밀도(Precision)과 재현율(Recall)의 조화 평균으로 계산.\n",
    " * mean_f1  : 다중 클래스 분류에서 모든 클래스에 대한 F1-Score의 평균값.\n",
    " * macro_f1 : 다중 클래스 분류에서 각 클래스별로 F1-Score를 계산한 후, 이를 평균한 값. 각 클래스의 중요성을 동일하게 고려하는 방식.\n",
    " * micro_f1 : 다중 클래스 분류에서 모든 클래스의 예측 결과를 합쳐서 하나의 이진 분류 문제로 취급. 모든 클래스의 예측 결과에 대해 균형있게 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5933333333333334 0.5523809523809523 0.6250000000000001\n",
      "0.5933333333333334 0.5523809523809523 0.6250000000000001\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# 다중 레이블 분류\n",
    "# -----------------------------------\n",
    "# mean_f1, macro_f1, micro_f1\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 다중 레이블 분류의 실젯값·예측값은 평가지표 계산상으로는 행 데이터 × 클래스의 두\n",
    "# 값 행렬로 해야 다루기 쉬움\n",
    "# 실젯값 - [[1,2], [1], [1,2,3], [2,3], [3]]\n",
    "y_true = np.array([[1, 1, 0],\n",
    "                   [1, 0, 0],\n",
    "                   [1, 1, 1],\n",
    "                   [0, 1, 1],\n",
    "                   [0, 0, 1]])\n",
    "\n",
    "# 예측값 - [[1,3], [2], [1,3], [3], [3]]\n",
    "y_pred = np.array([[1, 0, 1],\n",
    "                   [0, 1, 0],\n",
    "                   [1, 0, 1],\n",
    "                   [0, 0, 1],\n",
    "                   [0, 0, 1]])\n",
    "\n",
    "# mean-f1는 행 데이터마다 F1-score를 계산하여 평균을 취함\n",
    "mean_f1 = np.mean([f1_score(y_true[i, :], y_pred[i, :]) for i in range(len(y_true))])\n",
    "\n",
    "# macro-f1에서는 행 데이터마다 F1-score를 계산하여 평균을 취함\n",
    "n_class = 3\n",
    "macro_f1 = np.mean([f1_score(y_true[:, c], y_pred[:, c]) for c in range(n_class)])\n",
    "\n",
    "# micro-f1에서는 행 데이터 × 클래스의 쌍으로 TP/TN/FP/FN을 계산하여 F1-score를 구함\n",
    "micro_f1 = f1_score(y_true.reshape(-1), y_pred.reshape(-1))\n",
    "\n",
    "print(mean_f1, macro_f1, micro_f1)\n",
    "# 0.5933, 0.5524, 0.6250\n",
    "\n",
    "# scikit-learn 메소드를 사용하여 계산 가능\n",
    "mean_f1 = f1_score(y_true, y_pred, average='samples')\n",
    "macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "micro_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "print(mean_f1, macro_f1, micro_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분류 모델의 평가지표 - quadratic weighted kappa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* QWK(quadratic weighted kappa)는 분류 문제에서 예측된 레이블과 실제 레이블 간의 일치도를 측정하는 평가지표.\n",
    "* 0~1 사이의 범위를 갖는다. 값이 1에 가까울 수록 더 좋은 일치도를 나타낸다.\n",
    "   * 1 : 완벽한 일치\n",
    "   * 0.8 ~ 0.99 : 매우 높은 일치\n",
    "   * 0.6 ~ 0.79 : 상당한 일치\n",
    "   * 0.4 ~ 0.59 : 중간 정도 일치\n",
    "   * 0.2 ~ 0.39 : 낮은 일치\n",
    "   * 0 ~ 0.19 : 매우 낮은 일치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6153846153846154\n",
      "0.6153846153846154\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# 클래스간 순서관계가 있는 다중 클래스 분류\n",
    "# -----------------------------------\n",
    "# quadratic weighted kappa\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score\n",
    "\n",
    "# quadratic weighted kappa을 계산하는 함수\n",
    "def quadratic_weighted_kappa(c_matrix):\n",
    "    numer = 0.0\n",
    "    denom = 0.0\n",
    "\n",
    "    for i in range(c_matrix.shape[0]):\n",
    "        for j in range(c_matrix.shape[1]):\n",
    "            n = c_matrix.shape[0]\n",
    "            wij = ((i - j) ** 2.0)\n",
    "            oij = c_matrix[i, j]\n",
    "            eij = c_matrix[i, :].sum() * c_matrix[:, j].sum() / c_matrix.sum()\n",
    "            numer += wij * oij\n",
    "            denom += wij * eij\n",
    "\n",
    "    return 1.0 - numer / denom\n",
    "\n",
    "# y_true는 실젯값 클래스 목록, y_pred는 예측값 클래스 목록\n",
    "y_true = [1, 2, 3, 4, 3]\n",
    "y_pred = [2, 2, 4, 4, 5]\n",
    "\n",
    "# 혼동행렬을 계산\n",
    "c_matrix = confusion_matrix(y_true, y_pred, labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "# quadratic weighted kappa를 계산\n",
    "kappa = quadratic_weighted_kappa(c_matrix)\n",
    "print(kappa)\n",
    "# 0.6154 (소수점 5번째자리 반올림)\n",
    "\n",
    "# scikit-learn의 메소드로도 계산 가능\n",
    "kappa = cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "print(kappa)\n",
    "# 0.6154 (소수점 5번째자리 반올림)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분류의 평가지표 - MAP@K ( Mean Average Precision at K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MAP@K의 계산 방법\n",
    "  * 실제 레이블에 대한 예측 결과의 정렬된 리스트 생성\n",
    "  * Precision at K(P@K)값을 계산. P@K는 상위 K개의 예측 결과 중에서 실제 레이블이 있는 비율을 의미\n",
    "  * 각 검색 쿼리에 대한 P@K값을 모두 합산하여 평균을 계산. 이를 Average Precision(AP)라 한다. \n",
    "  * 모든 AP값을 평균내어 MAP@K를 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6499999999999999\n",
      "1.0\n",
      "0.5833333333333333\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# Recommendation(추천)\n",
    "# -----------------------------------\n",
    "# MAP@K\n",
    "\n",
    "# K=3、행의 수는 5개, 클래스는 4종류\n",
    "K = 3\n",
    "\n",
    "# 각 행의 실젯값\n",
    "y_true = [[1, 2], [1, 2], [4], [1, 2, 3, 4], [3, 4]]\n",
    "\n",
    "# 각 행에 대한 예측값 - K = 3이므로, 일반적으로 각 행에 각각 3개까지 순위를 매겨 예측\n",
    "y_pred = [[1, 2, 4], [4, 1, 2], [1, 4, 3], [1, 2, 3], [1, 2, 4]]\n",
    "\n",
    "# 각 행의 average precision을 계산하는 함수\n",
    "def apk(y_i_true, y_i_pred):\n",
    "    # y_pred가 K이하의 길이이고 모든 요소가 달라야 함\n",
    "    assert (len(y_i_pred) <= K)\n",
    "    assert (len(np.unique(y_i_pred)) == len(y_i_pred))\n",
    "\n",
    "    sum_precision = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i, p in enumerate(y_i_pred):\n",
    "        if p in y_i_true:\n",
    "            num_hits += 1\n",
    "            precision = num_hits / (i + 1)\n",
    "            sum_precision += precision\n",
    "\n",
    "    return sum_precision / min(len(y_i_true), K)\n",
    "\n",
    "# MAP@K을 계산하는 함수\n",
    "def mapk(y_true, y_pred):\n",
    "    return np.mean([apk(y_i_true, y_i_pred) for y_i_true, y_i_pred in zip(y_true, y_pred)])\n",
    "\n",
    "# MAP@K을 요청\n",
    "print(mapk(y_true, y_pred))\n",
    "# 0.65\n",
    "\n",
    "# 정답 수가 같아도 순서가 다르면 점수도 다름\n",
    "print(apk(y_true[0], y_pred[0]))\n",
    "print(apk(y_true[1], y_pred[1]))\n",
    "# 1.0, 0.5833"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
