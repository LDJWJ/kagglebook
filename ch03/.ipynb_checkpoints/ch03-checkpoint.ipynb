{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.17.0\n",
      "1.0.3\n",
      "0.22\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__)\n",
    "print(pd.__version__)\n",
    "print(sk.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 맞는 버전\n",
    " * numpy : 1.17\n",
    " * scikit learn : 0.22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### page 121\n",
    "\n",
    "```\n",
    "# 결측치를 지정하고 train.csv를 불러오기\n",
    "train = pd.read_csv('train.csv', na_values=['', 'NA', -1, 9999])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### page 122\n",
    "\n",
    "```\n",
    "# 열 col1의 값 -1을 결측치(nan)으로 변환\n",
    "train = pd.read_csv('train.csv', na_values=['', 'NA', -1, 9999])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------\n",
    "# データ等の準備\n",
    "# ----------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# train_xは学習データ、train_yは目的変数、test_xはテストデータ\n",
    "# pandasのDataFrame, Seriesで保持します。（numpyのarrayで保持することもあります）\n",
    "\n",
    "train = pd.read_csv('../input/sample-data/train_preprocessed.csv')\n",
    "train_x = train.drop(['target'], axis=1)\n",
    "train_y = train['target']\n",
    "test_x = pd.read_csv('../input/sample-data/test_preprocessed.csv')\n",
    "\n",
    "# 説明用に学習データとテストデータの元の状態を保存しておく\n",
    "train_x_saved = train_x.copy()\n",
    "test_x_saved = test_x.copy()\n",
    "\n",
    "\n",
    "# 学習データとテストデータを返す関数\n",
    "def load_data():\n",
    "    train_x, test_x = train_x_saved.copy(), test_x_saved.copy()\n",
    "    return train_x, test_x\n",
    "\n",
    "\n",
    "# 変換する数値変数をリストに格納\n",
    "num_cols = ['age', 'height', 'weight', 'amount',\n",
    "            'medical_info_a1', 'medical_info_a2', 'medical_info_a3', 'medical_info_b1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### page 124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 사전 준비\n",
    "# -----------------------------------\n",
    "# 표준화\n",
    "# -----------------------------------\n",
    "# 데이터 불러오기\n",
    "train_x, test_x = load_data()\n",
    "# -----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 학습 데이터를 기반으로 복수열의 표준화를 정의\n",
    "# 学習データに基づいて複数列の標準化を定義\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_x[num_cols])\n",
    "\n",
    "# 변환 후의 데이터를 컬럼명을 치환\n",
    "# 変換後のデータで各列を置換\n",
    "train_x[num_cols] = scaler.transform(train_x[num_cols])\n",
    "test_x[num_cols] = scaler.transform(test_x[num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### page 125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# 데이터 불러오기\n",
    "train_x, test_x = load_data()\n",
    "# -----------------------------------\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 학습데이터와 테스트 데이터를 결합한 데이터를 기준으로 복수 열의 표준화를 정의\n",
    "# 学習データとテストデータを結合したものに基づいて複数列の標準化を定義\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(pd.concat([train_x[num_cols], test_x[num_cols]]))\n",
    "\n",
    "# 변환 후, 데이터의 컬럼명을 치환\n",
    "# 変換後のデータで各列を置換\n",
    "train_x[num_cols] = scaler.transform(train_x[num_cols])\n",
    "test_x[num_cols] = scaler.transform(test_x[num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### page 126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# 데이터 불러오기\n",
    "train_x, test_x = load_data()\n",
    "# -----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 학습 데이터와 테스트 데이터를 각각 표준화(나쁜 예）\n",
    "scaler_train = StandardScaler()\n",
    "scaler_train.fit(train_x[num_cols])\n",
    "train_x[num_cols] = scaler_train.transform(train_x[num_cols])\n",
    "scaler_test = StandardScaler()\n",
    "scaler_test.fit(test_x[num_cols])\n",
    "test_x[num_cols] = scaler_test.transform(test_x[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Min-Max 스케일링(scaling)\n",
    "# -----------------------------------\n",
    "# データの読み込み\n",
    "train_x, test_x = load_data()\n",
    "# -----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 학습 데이터를 기반으로 여러 열의 Min-Max 스케일링을 정의\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_x[num_cols])\n",
    "\n",
    "# 변환 후의 데이터로 각 열을 치환\n",
    "train_x[num_cols] = scaler.transform(train_x[num_cols])\n",
    "test_x[num_cols] = scaler.transform(test_x[num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3 비선형 변환(p127~ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# 로그 변환\n",
    "# -----------------------------------\n",
    "x = np.array([1.0, 10.0, 100.0, 1000.0, 10000.0])\n",
    "\n",
    "# 간단하게 로그를 취함.\n",
    "x1 = np.log(x)\n",
    "\n",
    "# 1을 더한 후에 로그를 취함\n",
    "x2 = np.log1p(x)\n",
    "\n",
    "# 절대값의 로그를 취하고, 원래의 부호를 취한다.\n",
    "x3 = np.sign(x) * np.log(np.abs(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Box-Cox 변환\n",
    "# -----------------------------------\n",
    "# 데이터의 불러오기\n",
    "train_x, test_x = load_data()\n",
    "# -----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 양의 값만을 취하는 변수를 변환 대상으로 하여 리스트에 추가한다.\n",
    "# 또한 결측치도 포함하는 경우에는 (~)(train_x[c] <=0.0).all() 등으로 할 필요가 있으므로 주의\n",
    "pos_cols = [c for c in num_cols if (train_x[c] > 0.0).all() and (test_x[c] > 0.0).all()]\n",
    "\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# 학습 데이터를 기반으로 복수열의 Box-Cox 변환을 정의\n",
    "pt = PowerTransformer(method='box-cox')\n",
    "pt.fit(train_x[pos_cols])\n",
    "\n",
    "# 변환 후의 데이터로 각 열 치환\n",
    "train_x[pos_cols] = pt.transform(train_x[pos_cols])\n",
    "test_x[pos_cols] = pt.transform(test_x[pos_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Yeo-Johnson 변환\n",
    "# -----------------------------------\n",
    "# 데이터 불러오기\n",
    "train_x, test_x = load_data()\n",
    "# -----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# 학습 데이터를 기반으로 복수열의 Yeo-Johnson 변환을 정의\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "pt.fit(train_x[num_cols])\n",
    "\n",
    "# 변환후의 데이터로 각 열을 치환\n",
    "train_x[num_cols] = pt.transform(train_x[num_cols])\n",
    "test_x[num_cols] = pt.transform(test_x[num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.4 clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# clipping\n",
    "# -----------------------------------\n",
    "# 데이터 불러오기\n",
    "train_x, test_x = load_data()\n",
    "# -----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 열마다 학습 데이터의 1%점, 99%점 계산\n",
    "p01 = train_x[num_cols].quantile(0.01)\n",
    "p99 = train_x[num_cols].quantile(0.99)\n",
    "\n",
    "# 1％ 점 이하의 값은 1%점에, 99%점 이상의 값은 99%점에 clipping 한다.\n",
    "train_x[num_cols] = train_x[num_cols].clip(p01, p99, axis=1)\n",
    "test_x[num_cols] = test_x[num_cols].clip(p01, p99, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.5 binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 1 1 2 0]\n",
      "[0 2 1 1 2 0]\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# binning\n",
    "# -----------------------------------\n",
    "x = [1, 7, 5, 4, 6, 3]\n",
    "\n",
    "# pandas의 cut함수로 binning을 수행한다.\n",
    "\n",
    "# bin의 수를 지정하는 경우,\n",
    "binned = pd.cut(x, 3, labels=False)\n",
    "print(binned)\n",
    "# [0 2 1 1 2 0] - 변환된 값은 3개의 bin 중에 어떤 것에 포함되었는지 나타냄.\n",
    "\n",
    "# bin의 범위를 지정하는 경우\n",
    "# (3.0이하, 3.0보다 크게 5.0이하, 5.0보다 큼)\n",
    "bin_edges = [-float('inf'), 3.0, 5.0, float('inf')]\n",
    "binned = pd.cut(x, bin_edges, labels=False)\n",
    "print(binned)\n",
    "# [0 2 1 1 2 0] - 변환된 값은 3개의 bin 중 어느 것에 들어갔는지를 나타냄."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.  3.  4.  1.  5.5 5.5]\n",
      "[1 2 3 0 4 5]\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# 순위의 변환\n",
    "# -----------------------------------\n",
    "x = [10, 20, 30, 0, 40, 40]\n",
    "\n",
    "# pandas의 rank함수로 순위를 변환\n",
    "rank = pd.Series(x).rank()\n",
    "print(rank.values)\n",
    "# 처음이 1, 같은 순위가 있을 경우, 평균 순위가 된다.\n",
    "# [2. 3. 4. 1. 5.5 5.5]\n",
    "\n",
    "# numpy의 argsort함수를 2회 적용하는 방법으로 순위를 변환\n",
    "order = np.argsort(x)\n",
    "rank = np.argsort(order)\n",
    "print(rank)\n",
    "# 시작이 0、같은 순위가 있는 경우는 어느쪽이든 상위가 된다.\n",
    "# [1 2 3 0 4 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# RankGauss\n",
    "# -----------------------------------\n",
    "# 데이터 불러오기\n",
    "train_x, test_x = load_data()\n",
    "# -----------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "# 학습 데이터를 기반으로 여러 열의 Rank Gauss를 통한 변환을 정의\n",
    "transformer = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution='normal')\n",
    "transformer.fit(train_x[num_cols])\n",
    "\n",
    "# 변환 후의 데이터로 각 열을 치환\n",
    "train_x[num_cols] = transformer.transform(train_x[num_cols])\n",
    "test_x[num_cols] = transformer.transform(test_x[num_cols])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 범주형 변수의 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사전에 필요한 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------\n",
    "# データ等の準備\n",
    "# ----------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# train_xは学習データ、train_yは目的変数、test_xはテストデータ\n",
    "# pandasのDataFrame, Seriesで保持します。（numpyのarrayで保持することもあります）\n",
    "\n",
    "train = pd.read_csv('../input/sample-data/train.csv')\n",
    "train_x = train.drop(['target'], axis=1)\n",
    "train_y = train['target']\n",
    "test_x = pd.read_csv('../input/sample-data/test.csv')\n",
    "\n",
    "# 説明用に学習データとテストデータの元の状態を保存しておく\n",
    "train_x_saved = train_x.copy()\n",
    "test_x_saved = test_x.copy()\n",
    "\n",
    "\n",
    "# 学習データとテストデータを返す関数\n",
    "def load_data():\n",
    "    train_x, test_x = train_x_saved.copy(), test_x_saved.copy()\n",
    "    return train_x, test_x\n",
    "\n",
    "\n",
    "# 変換するカテゴリ変数をリストに格納\n",
    "cat_cols = ['sex', 'product', 'medical_info_b2', 'medical_info_b3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# one-hot encoding\n",
    "# -----------------------------------\n",
    "# データの読み込み\n",
    "train_x, test_x = load_data()\n",
    "# -----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習データとテストデータを結合してget_dummiesによるone-hot encodingを行う\n",
    "all_x = pd.concat([train_x, test_x])\n",
    "all_x = pd.get_dummies(all_x, columns=cat_cols)\n",
    "\n",
    "# 学習データとテストデータに再分割\n",
    "train_x = all_x.iloc[:train_x.shape[0], :].reset_index(drop=True)\n",
    "test_x = all_x.iloc[train_x.shape[0]:, :].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# データの読み込み\n",
    "train_x, test_x = load_data()\n",
    "# -----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# OneHotEncoderでのencoding\n",
    "ohe = OneHotEncoder(sparse=False, categories='auto')\n",
    "ohe.fit(train_x[cat_cols])\n",
    "\n",
    "# ダミー変数の列名の作成\n",
    "columns = []\n",
    "for i, c in enumerate(cat_cols):\n",
    "    columns += [f'{c}_{v}' for v in ohe.categories_[i]]\n",
    "\n",
    "# 生成されたダミー変数をデータフレームに変換\n",
    "dummy_vals_train = pd.DataFrame(ohe.transform(train_x[cat_cols]), columns=columns)\n",
    "dummy_vals_test = pd.DataFrame(ohe.transform(test_x[cat_cols]), columns=columns)\n",
    "\n",
    "# 残りの変数と結合\n",
    "train_x = pd.concat([train_x.drop(cat_cols, axis=1), dummy_vals_train], axis=1)\n",
    "test_x = pd.concat([test_x.drop(cat_cols, axis=1), dummy_vals_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2 label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# label encoding\n",
    "# -----------------------------------\n",
    "# データの読み込み\n",
    "train_x, test_x = load_data()\n",
    "# -----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# カテゴリ変数をループしてlabel encoding\n",
    "for c in cat_cols:\n",
    "    # 学習データに基づいて定義する\n",
    "    le = LabelEncoder()\n",
    "    le.fit(train_x[c])\n",
    "    train_x[c] = le.transform(train_x[c])\n",
    "    test_x[c] = le.transform(test_x[c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.3 featuer hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# feature hashing\n",
    "# -----------------------------------\n",
    "# データの読み込み\n",
    "train_x, test_x = load_data()\n",
    "# -----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "# カテゴリ変数をループしてfeature hashing\n",
    "for c in cat_cols:\n",
    "    # FeatureHasherの使い方は、他のencoderとは少し異なる\n",
    "\n",
    "    fh = FeatureHasher(n_features=5, input_type='string')\n",
    "    # 変数を文字列に変換してからFeatureHasherを適用\n",
    "    hash_train = fh.transform(train_x[[c]].astype(str).values)\n",
    "    hash_test = fh.transform(test_x[[c]].astype(str).values)\n",
    "    # データフレームに変換\n",
    "    hash_train = pd.DataFrame(hash_train.todense(), columns=[f'{c}_{i}' for i in range(5)])\n",
    "    hash_test = pd.DataFrame(hash_test.todense(), columns=[f'{c}_{i}' for i in range(5)])\n",
    "    # 元のデータフレームと結合\n",
    "    train_x = pd.concat([train_x, hash_train], axis=1)\n",
    "    test_x = pd.concat([test_x, hash_test], axis=1)\n",
    "\n",
    "# 元のカテゴリ変数を削除\n",
    "train_x.drop(cat_cols, axis=1, inplace=True)\n",
    "test_x.drop(cat_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.4 frequency encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# frequency encoding\n",
    "# -----------------------------------\n",
    "# データの読み込み\n",
    "train_x, test_x = load_data()\n",
    "# -----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 変数をループしてfrequency encoding\n",
    "for c in cat_cols:\n",
    "    freq = train_x[c].value_counts()\n",
    "    # カテゴリの出現回数で置換\n",
    "    train_x[c] = train_x[c].map(freq)\n",
    "    test_x[c] = test_x[c].map(freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.5 target encoing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# target encoding\n",
    "# -----------------------------------\n",
    "# データの読み込み\n",
    "train_x, test_x = load_data()\n",
    "# -----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# 変数をループしてtarget encoding\n",
    "for c in cat_cols:\n",
    "    # 学習データ全体で各カテゴリにおけるtargetの平均を計算\n",
    "    data_tmp = pd.DataFrame({c: train_x[c], 'target': train_y})\n",
    "    target_mean = data_tmp.groupby(c)['target'].mean()\n",
    "    # テストデータのカテゴリを置換\n",
    "    test_x[c] = test_x[c].map(target_mean)\n",
    "\n",
    "    # 学習データの変換後の値を格納する配列を準備\n",
    "    tmp = np.repeat(np.nan, train_x.shape[0])\n",
    "\n",
    "    # 学習データを分割\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=72)\n",
    "    for idx_1, idx_2 in kf.split(train_x):\n",
    "        # out-of-foldで各カテゴリにおける目的変数の平均を計算\n",
    "        target_mean = data_tmp.iloc[idx_1].groupby(c)['target'].mean()\n",
    "        # 変換後の値を一時配列に格納\n",
    "        tmp[idx_2] = train_x[c].iloc[idx_2].map(target_mean)\n",
    "\n",
    "    # 変換後のデータで元の変数を置換\n",
    "    train_x[c] = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### page 146"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# target encoding - クロスバリデーションのfoldごとの場合\n",
    "# -----------------------------------\n",
    "# データの読み込み\n",
    "train_x, test_x = load_data()\n",
    "# -----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# クロスバリデーションのfoldごとにtarget encodingをやり直す\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=71)\n",
    "for i, (tr_idx, va_idx) in enumerate(kf.split(train_x)):\n",
    "\n",
    "    # 学習データからバリデーションデータを分ける\n",
    "    tr_x, va_x = train_x.iloc[tr_idx].copy(), train_x.iloc[va_idx].copy()\n",
    "    tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n",
    "\n",
    "    # 変数をループしてtarget encoding\n",
    "    for c in cat_cols:\n",
    "        # 学習データ全体で各カテゴリにおけるtargetの平均を計算\n",
    "        data_tmp = pd.DataFrame({c: tr_x[c], 'target': tr_y})\n",
    "        target_mean = data_tmp.groupby(c)['target'].mean()\n",
    "        # バリデーションデータのカテゴリを置換\n",
    "        va_x.loc[:, c] = va_x[c].map(target_mean)\n",
    "\n",
    "        # 学習データの変換後の値を格納する配列を準備\n",
    "        tmp = np.repeat(np.nan, tr_x.shape[0])\n",
    "        kf_encoding = KFold(n_splits=4, shuffle=True, random_state=72)\n",
    "        for idx_1, idx_2 in kf_encoding.split(tr_x):\n",
    "            # out-of-foldで各カテゴリにおける目的変数の平均を計算\n",
    "            target_mean = data_tmp.iloc[idx_1].groupby(c)['target'].mean()\n",
    "            # 変換後の値を一時配列に格納\n",
    "            tmp[idx_2] = tr_x[c].iloc[idx_2].map(target_mean)\n",
    "\n",
    "        tr_x.loc[:, c] = tmp\n",
    "\n",
    "    # 必要に応じてencodeされた特徴量を保存し、あとで読み込めるようにしておく"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### page 147"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# target encoding - クロスバリデーションのfoldとtarget encodingのfoldの分割を合わせる場合\n",
    "# -----------------------------------\n",
    "# データの読み込み\n",
    "train_x, test_x = load_data()\n",
    "# -----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# クロスバリデーションのfoldを定義\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=71)\n",
    "\n",
    "# 変数をループしてtarget encoding\n",
    "for c in cat_cols:\n",
    "\n",
    "    # targetを付加\n",
    "    data_tmp = pd.DataFrame({c: train_x[c], 'target': train_y})\n",
    "    # 変換後の値を格納する配列を準備\n",
    "    tmp = np.repeat(np.nan, train_x.shape[0])\n",
    "\n",
    "    # 学習データからバリデーションデータを分ける\n",
    "    for i, (tr_idx, va_idx) in enumerate(kf.split(train_x)):\n",
    "        # 学習データについて、各カテゴリにおける目的変数の平均を計算\n",
    "        target_mean = data_tmp.iloc[tr_idx].groupby(c)['target'].mean()\n",
    "        # バリデーションデータについて、変換後の値を一時配列に格納\n",
    "        tmp[va_idx] = train_x[c].iloc[va_idx].map(target_mean)\n",
    "\n",
    "    # 変換後のデータで元の変数を置換\n",
    "    train_x[c] = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 변수의 조합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------------\n",
    "# データの結合\n",
    "# -----------------------------------\n",
    "# データの読み込み\n",
    "train = pd.read_csv('../input/ch03/multi_table_train.csv')\n",
    "product_master = pd.read_csv('../input/ch03/multi_table_product.csv')\n",
    "user_log = pd.read_csv('../input/ch03/multi_table_log.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# 図の形式のデータフレームがあるとする\n",
    "# train         : 学習データ（ユーザID, 商品ID, 目的変数などの列がある）\n",
    "# product_master: 商品マスタ（商品IDと商品の情報を表す列がある）\n",
    "# user_log      : ユーザの行動のログデータ（ユーザIDと各行動の情報を表す列がある）\n",
    "\n",
    "# 商品マスタを学習データと結合する\n",
    "train = train.merge(product_master, on='product_id', how='left')\n",
    "\n",
    "# ログデータのユーザごとの行数を集計し、学習データと結合する\n",
    "user_log_agg = user_log.groupby('user_id').size().reset_index().rename(columns={0: 'user_count'})\n",
    "train = train.merge(user_log_agg, on='user_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.10.3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              A     B     C\n",
      "2016-07-01  532  3314  1136\n",
      "2016-07-02  798  2461  1188\n",
      "2016-07-03  823  3522  1711\n",
      "2016-07-04  937  5451  1977\n",
      "2016-07-05  881  4729  1975\n",
      "           id  value\n",
      "2016-07-01  A    532\n",
      "2016-07-01  B   3314\n",
      "2016-07-01  C   1136\n",
      "2016-07-02  A    798\n",
      "2016-07-02  B   2461\n",
      "2016-07-02  C   1188\n",
      "2016-07-03  A    823\n",
      "2016-07-03  B   3522\n",
      "2016-07-03  C   1711\n",
      "2016-07-04  A    937\n"
     ]
    }
   ],
   "source": [
    "# ワイドフォーマットのデータを読み込む\n",
    "df_wide = pd.read_csv('../input/ch03/time_series_wide.csv', index_col=0)\n",
    "# インデックスの型を日付型に変更する\n",
    "df_wide.index = pd.to_datetime(df_wide.index)\n",
    "\n",
    "print(df_wide.iloc[:5, :3])\n",
    "'''\n",
    "              A     B     C\n",
    "date\n",
    "2016-07-01  532  3314  1136\n",
    "2016-07-02  798  2461  1188\n",
    "2016-07-03  823  3522  1711\n",
    "2016-07-04  937  5451  1977\n",
    "2016-07-05  881  4729  1975\n",
    "'''\n",
    "\n",
    "# ロングフォーマットに変換する\n",
    "df_long = df_wide.stack().reset_index(1)\n",
    "df_long.columns = ['id', 'value']\n",
    "\n",
    "print(df_long.head(10))\n",
    "'''\n",
    "           id  value\n",
    "date\n",
    "2016-07-01  A    532\n",
    "2016-07-01  B   3314\n",
    "2016-07-01  C   1136\n",
    "2016-07-02  A    798\n",
    "2016-07-02  B   2461\n",
    "2016-07-02  C   1188\n",
    "2016-07-03  A    823\n",
    "2016-07-03  B   3522\n",
    "2016-07-03  C   1711\n",
    "2016-07-04  A    937\n",
    "...\n",
    "'''\n",
    "\n",
    "# ワイドフォーマットに戻す\n",
    "df_wide = df_long.pivot(index=None, columns='id', values='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.10.4 lag 종속변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# ラグ変数\n",
    "# -----------------------------------\n",
    "# ワイドフォーマットのデータをセットする\n",
    "x = df_wide\n",
    "# -----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xはワイドフォーマットのデータフレーム\n",
    "# インデックスが日付などの時間、列がユーザや店舗などで、値が売上などの注目する変数を表すものとする\n",
    "\n",
    "# 1期前のlagを取得\n",
    "x_lag1 = x.shift(1)\n",
    "\n",
    "# 7期前のlagを取得\n",
    "x_lag7 = x.shift(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1期前から3期間の移動平均を算出\n",
    "x_avg3 = x.shift(1).rolling(window=3).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1期前から7期間の最大値を算出\n",
    "x_max7 = x.shift(1).rolling(window=7).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7期前, 14期前, 21期前, 28期前の値の平均\n",
    "x_e7_avg = (x.shift(7) + x.shift(14) + x.shift(21) + x.shift(28)) / 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1期先の値を取得\n",
    "x_lead1 = x.shift(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.10.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# ラグ変数\n",
    "# -----------------------------------\n",
    "# データの読み込み\n",
    "train_x = pd.read_csv('../input/ch03/time_series_train.csv')\n",
    "event_history = pd.read_csv('../input/ch03/time_series_events.csv')\n",
    "train_x['date'] = pd.to_datetime(train_x['date'])\n",
    "event_history['date'] = pd.to_datetime(event_history['date'])\n",
    "# -----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_xは学習データで、ユーザID, 日付を列として持つDataFrameとする\n",
    "# event_historyは、過去に開催したイベントの情報で、日付、イベントを列として持つDataFrameとする\n",
    "\n",
    "# occurrencesは、日付、セールが開催されたか否かを列として持つDataFrameとなる\n",
    "dates = np.sort(train_x['date'].unique())\n",
    "occurrences = pd.DataFrame(dates, columns=['date'])\n",
    "sale_history = event_history[event_history['event'] == 'sale']\n",
    "occurrences['sale'] = occurrences['date'].isin(sale_history['date'])\n",
    "\n",
    "# 累積和をとることで、それぞれの日付での累積出現回数を表すようにする\n",
    "# occurrencesは、日付、セールの累積出現回数を列として持つDataFrameとなる\n",
    "occurrences['sale'] = occurrences['sale'].cumsum()\n",
    "\n",
    "# 日付をキーとして学習データと結合する\n",
    "train_x = train_x.merge(occurrences, on='date', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.11 차원감소, 비지도학습학습에 따른 피처 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사전 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------\n",
    "# データ等の準備\n",
    "# ----------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# train_xは学習データ、train_yは目的変数、test_xはテストデータ\n",
    "# pandasのDataFrame, Seriesで保持します。（numpyのarrayで保持することもあります）\n",
    "\n",
    "train = pd.read_csv('../input/sample-data/train_preprocessed_onehot.csv')\n",
    "train_x = train.drop(['target'], axis=1)\n",
    "train_y = train['target']\n",
    "test_x = pd.read_csv('../input/sample-data/test_preprocessed_onehot.csv')\n",
    "\n",
    "# 説明用に学習データとテストデータの元の状態を保存しておく\n",
    "train_x_saved = train_x.copy()\n",
    "test_x_saved = test_x.copy()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "# 標準化を行った学習データとテストデータを返す関数\n",
    "def load_standarized_data():\n",
    "    train_x, test_x = train_x_saved.copy(), test_x_saved.copy()\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_x)  # numpy version 1.18 error발생 \n",
    "    train_x = scaler.transform(train_x)\n",
    "    test_x = scaler.transform(test_x)\n",
    "    return pd.DataFrame(train_x), pd.DataFrame(test_x)\n",
    "\n",
    "\n",
    "# MinMaxスケーリングを行った学習データとテストデータを返す関数\n",
    "def load_minmax_scaled_data():\n",
    "    train_x, test_x = train_x_saved.copy(), test_x_saved.copy()\n",
    "\n",
    "    # Min-Max Scalingを行う\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(pd.concat([train_x, test_x], axis=0))\n",
    "    train_x = scaler.transform(train_x)\n",
    "    test_x = scaler.transform(test_x)\n",
    "\n",
    "    return pd.DataFrame(train_x), pd.DataFrame(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 59 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   age                  10000 non-null  int64  \n",
      " 1   sex                  10000 non-null  int64  \n",
      " 2   height               10000 non-null  float64\n",
      " 3   weight               10000 non-null  float64\n",
      " 4   product_0            10000 non-null  int64  \n",
      " 5   product_1            10000 non-null  int64  \n",
      " 6   product_2            10000 non-null  int64  \n",
      " 7   product_3            10000 non-null  int64  \n",
      " 8   product_4            10000 non-null  int64  \n",
      " 9   product_5            10000 non-null  int64  \n",
      " 10  product_6            10000 non-null  int64  \n",
      " 11  product_7            10000 non-null  int64  \n",
      " 12  product_8            10000 non-null  int64  \n",
      " 13  product_9            10000 non-null  int64  \n",
      " 14  product_10           10000 non-null  int64  \n",
      " 15  amount               10000 non-null  int64  \n",
      " 16  medical_info_a1      10000 non-null  int64  \n",
      " 17  medical_info_a2      10000 non-null  int64  \n",
      " 18  medical_info_a3      10000 non-null  int64  \n",
      " 19  medical_info_b1      10000 non-null  int64  \n",
      " 20  medical_info_b2_0    10000 non-null  int64  \n",
      " 21  medical_info_b2_1    10000 non-null  int64  \n",
      " 22  medical_info_b2_2    10000 non-null  int64  \n",
      " 23  medical_info_b2_3    10000 non-null  int64  \n",
      " 24  medical_info_b3_0    10000 non-null  int64  \n",
      " 25  medical_info_b3_1    10000 non-null  int64  \n",
      " 26  medical_info_b3_2    10000 non-null  int64  \n",
      " 27  medical_info_b3_3    10000 non-null  int64  \n",
      " 28  medical_info_b3_4    10000 non-null  int64  \n",
      " 29  medical_info_b3_5    10000 non-null  int64  \n",
      " 30  medical_info_b3_6    10000 non-null  int64  \n",
      " 31  medical_info_b3_7    10000 non-null  int64  \n",
      " 32  medical_info_b3_8    10000 non-null  int64  \n",
      " 33  medical_info_b3_9    10000 non-null  int64  \n",
      " 34  medical_info_b3_10   10000 non-null  int64  \n",
      " 35  medical_info_b3_11   10000 non-null  int64  \n",
      " 36  medical_info_b3_12   10000 non-null  int64  \n",
      " 37  medical_info_b3_13   10000 non-null  int64  \n",
      " 38  medical_info_b3_14   10000 non-null  int64  \n",
      " 39  medical_info_b3_15   10000 non-null  int64  \n",
      " 40  medical_info_b3_16   10000 non-null  int64  \n",
      " 41  medical_info_c1      10000 non-null  float64\n",
      " 42  medical_info_c2      10000 non-null  float64\n",
      " 43  medical_keyword_1    10000 non-null  int64  \n",
      " 44  medical_keyword_2    10000 non-null  int64  \n",
      " 45  medical_keyword_3    10000 non-null  int64  \n",
      " 46  medical_keyword_4    10000 non-null  int64  \n",
      " 47  medical_keyword_5    10000 non-null  int64  \n",
      " 48  medical_keyword_6    10000 non-null  int64  \n",
      " 49  medical_keyword_7    10000 non-null  int64  \n",
      " 50  medical_keyword_8    10000 non-null  int64  \n",
      " 51  medical_keyword_9    10000 non-null  int64  \n",
      " 52  medical_keyword_10   10000 non-null  int64  \n",
      " 53  year                 10000 non-null  int64  \n",
      " 54  month                10000 non-null  int64  \n",
      " 55  day                  10000 non-null  int64  \n",
      " 56  yearmonth            10000 non-null  int64  \n",
      " 57  medical_info_c1_nan  10000 non-null  bool   \n",
      " 58  medical_info_c2_nan  10000 non-null  bool   \n",
      "dtypes: bool(2), float64(4), int64(53)\n",
      "memory usage: 4.4 MB\n"
     ]
    }
   ],
   "source": [
    "train_x1, test_x1 = train_x_saved.copy(), test_x_saved.copy()\n",
    "scaler = StandardScaler()\n",
    "train_x1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age\n",
      "[50 68 77 17 62 14 63 42  9 40 54  5 44 55 67 33 43 11 56 35  8 46 36 28\n",
      " 39  6 27 34 18 73 37 65 47 51 25 66 12  7 45 49 59 53 64 78 13 10 52 71\n",
      " 31 20 41 23 30 16 21 24 29 22 57 60 38 76 19 79 15 32 74 70 58 48 26 61\n",
      " 72 75 69]\n",
      "sex\n",
      "[1 0]\n",
      "height\n",
      "[166.44560805 164.33461501 167.46291654 ... 145.60999802 165.79601735\n",
      " 180.30176166]\n",
      "weight\n",
      "[65.01673234 56.54421749 54.24226666 ... 47.73939704 57.56769457\n",
      " 71.42513527]\n",
      "product_0\n",
      "[0 1]\n",
      "product_1\n",
      "[0 1]\n",
      "product_2\n",
      "[0 1]\n",
      "product_3\n",
      "[0 1]\n",
      "product_4\n",
      "[0 1]\n",
      "product_5\n",
      "[0 1]\n",
      "product_6\n",
      "[0 1]\n",
      "product_7\n",
      "[0 1]\n",
      "product_8\n",
      "[0 1]\n",
      "product_9\n",
      "[1 0]\n",
      "product_10\n",
      "[0 1]\n",
      "amount\n",
      "[ 7000000  6000000  8000000  9000000    10000  3000000     2000     6000\n",
      "  5000000  2000000  1000000  4000000     5000     4000     1000 10000000\n",
      "     3000     7000     9000     8000]\n",
      "medical_info_a1\n",
      "[134 438 313 342 327 389  57 307 201 209 349 284 441 432 397 341 289 193\n",
      " 476 292 323 399 250 410 325 371 360 303 321 332 318 230 136 252 421 248\n",
      " 494 287 131 210 285 330 146 305 295 196 240  95 191 405 260 127 275 311\n",
      " 402 339 150  94 181 297 170 267 225 113 403 440 166 245  42 104 180 329\n",
      " 420  76 506 545 293 176 312 309 109 308 390 273 205 194 336 189 450 589\n",
      " 253 239 255 153 434 347 251 404 233 394 320 254 406 291 198 279 175 160\n",
      " 373 376 281 433 263 133 276 280   0 152 386 135 352 185 222 367 236 464\n",
      " 294 418 357 302 182 338 416 384  61 414 103 211 197 324 247 317 199 296\n",
      " 595 214 467 346 269 243 379 257 375 429 452 208 351 437 502 515 343 480\n",
      " 422 139 262 138 319 224 407 469 388 165 411 220 401 144 335 348 119 413\n",
      " 366 188 200 234 363 322 227 374 232 159 173 278 178 344 355 105 145 378\n",
      " 246 212 556 277 235 261 445 167 288 192 391 364 258 430  38 350 141 514\n",
      " 217 426 249 475 147 216 238 417 283 409 395 396 143 345 530 298 444 537\n",
      " 242 162 244 -56 383 237 446 183 259 228 521 381 219  98 525 435 362 385\n",
      " 110 231 164 387 206 272 358 419 310 186 202 149 129 151 481 400 492 497\n",
      " 290 461 333 179 314 326 187 456 264 415 527 425 455 453 100 488 315 223\n",
      " 334 354 207 483 356 268 226 114 474 370 140 353 306 602 566 580 215  54\n",
      " 505 361 107 190 125 204 304 213  87 458 457 517 408 504 128 479 570 393\n",
      "  70 126 316 471 382 546 337 265 328 156 340  56 463 300 137 513 477 489\n",
      " 266 155 270 427 301 218 447 482 442 299 507 451 553  66 168 431 368 241\n",
      " 377 286 510 195 221 111 142  93 118 511 122 331 503  15 171 177 462 468\n",
      " 372 412 524 380 369 163 274  48 569 271  69  58 478 485 392 631 501 448\n",
      " 436 493  99 132 184  89 172 520 498 117 423 203  96 229 130  65 101 465\n",
      "  82 554 154  92 550 121 472 282 512 573 116 428 424 532  60 539 174  74\n",
      " 439  13 359 547 663 519  46 157 560 161 256  23 486 -13   9  45 106 490\n",
      " 473  64 533 449 500 594 365 460 -39 158 542  83  84 617 102 557 549 484\n",
      "  91 120 587 582 526 112 459 443 398 -58 454  14  81 543 466  63 563 496\n",
      " 564  31  88  49 108 499 124 495 579 169 -62 115  75 -44  32  43  73 148\n",
      " 559  47  19 565  21 508 470 491  30 548 552 487  -8  -4  59 588 590  78\n",
      " 123  27 522 648  34 558 555  17  55 538  33 551 518  86 531 634 605   7\n",
      "  53  18 567 584 597 528  39  68 598 627 562  97 585  90   5  24  52 581\n",
      "  50  37  80 540 529 534 535  72  79 706 536 625 641  62  -5 574  71 599\n",
      " 561 523 509  51 571  77  22 572  12  29  67  11 -52 624   1  41 607   3\n",
      "  36 639 -16 -50 629  -9 645  44]\n",
      "medical_info_a2\n",
      "[ 202  263  325  213  102  229  344  315  393  226  348  266  160  224\n",
      "  447  347  251  190   63  246  352  288  142  162  183  280   45   79\n",
      "   77  311  187  336  128  255   47  177   17  326  285  210  149  135\n",
      "  141  145  294  316  378  218  332  402  291  217   34  204  241  265\n",
      "  271  245  239  379  279   87  374  225  200  296  380  398  299  346\n",
      "  170  -15  -11   -9  107  349   21  381  192  476  180  197   68   24\n",
      "  385   28   23   26  327  438  238  250  123  171  191  121  485  247\n",
      "  147  275  345  163  474  234  397  153  418  353  148  431  258  282\n",
      "  340   90  507  212  491  323  267  256  410  115  370  383  274  335\n",
      "  377  305  259  287  182  362  119   -1  304  156  321  152  400  433\n",
      "  235  416  151  112  356   97  289  109  205  399  463  252  185  144\n",
      "  221  272  173  354  429  140  312  260  365  166  425  284  248  357\n",
      "  360  261  436  278  169  203  387  375   82  254  497   46   91  199\n",
      "  333  223  207  231  471  175  220  159  227  371  465  146  419  186\n",
      "  211  303  106    0   89  422  237  188  276  150  193  249  322  328\n",
      "  446  179  243  167  359  122  172  368  158   80  222  281  155  113\n",
      "  198  138  401  297  290  443  372  189  407   40  373  295  116  341\n",
      "  164  477  208  367   92  324  157  232  318  293  309  196   71  364\n",
      "  242  363  240   58  319  270  244   60  453  195  257  396  216  506\n",
      "   88   42  313  391  376  343  434  308  481  534  214  301  233  133\n",
      "  286  298  253  181  201   69  262    7   83  215  470  395   41  168\n",
      "   67   73  174  413  134  300  437  273   96  314  310  277  104  124\n",
      "  317  228   27  136  350  334  330  307  117  337   95  206  358    2\n",
      "  219  382  283  386  -19  576  165  292   19  306  351   86  236  394\n",
      "  456 -125  127  441  110   99  194   39  161  361   55  417  366   85\n",
      "  432  329  126  209  342  130  269  448   66  268  369  338  184  459\n",
      "   76  302  230  264  405  355   98  452  120  392  331  131  -50  143\n",
      "  389  111   57  320  426  439  176  154  409   74  137  479  406  139\n",
      "  -27   38  103  563   94  421   50  100  468  178  -40  388  564   65\n",
      "  415   20  384  435  604   93  423  118  339  -16  428   18    5   56\n",
      "  504   36   84  412  -30   75   33  464  414  458  509    4  457  490\n",
      "  466  404  531  444  420  390   22  430  403   53  516  105  129   72\n",
      "  473  -34  621   78  440  114   16  408  478   48  108   81   61  -69\n",
      "   31   15   64  445  -73   -3   70  -14  -24  411  558  532   49   43\n",
      "  427  540  450  482  484  132  535  -42   35   29  -37   59 -114  451\n",
      "  461  -36  511   -6  449   -5  467   62 -112  544  494    3   14  424\n",
      "   10  125   30  492   52  -22  510  -56    6  487   -2  101   11  493\n",
      "  462  -38   51  475  472  568    8  -39  483  512   44  442  499  514\n",
      "  -13  527  530  -61  541   -8  454  500  515  -17  -12  460  455   25\n",
      "  574  -35  529   -4  619  630   12  636  590  538    1  582  498  -32\n",
      "   37  519  552  555  528  542  495  -31  533  -75 -137   13  521  -10\n",
      "  -48  556  -25  -49  524  526  513  545  517  488  539  -18  502  536\n",
      "  -29  -93    9  -57  -26  656  -68  -52  525   54   32  469  548]\n",
      "medical_info_a3\n",
      "[1 3 2 0 4 7 9 5 6 8]\n",
      "medical_info_b1\n",
      "[11 14 18 15 13 16 19 12 17 10]\n",
      "medical_info_b2_0\n",
      "[0 1]\n",
      "medical_info_b2_1\n",
      "[0 1]\n",
      "medical_info_b2_2\n",
      "[1 0]\n",
      "medical_info_b2_3\n",
      "[0 1]\n",
      "medical_info_b3_0\n",
      "[0 1]\n",
      "medical_info_b3_1\n",
      "[1 0]\n",
      "medical_info_b3_2\n",
      "[0 1]\n",
      "medical_info_b3_3\n",
      "[0 1]\n",
      "medical_info_b3_4\n",
      "[0 1]\n",
      "medical_info_b3_5\n",
      "[0 1]\n",
      "medical_info_b3_6\n",
      "[0 1]\n",
      "medical_info_b3_7\n",
      "[0 1]\n",
      "medical_info_b3_8\n",
      "[0 1]\n",
      "medical_info_b3_9\n",
      "[0 1]\n",
      "medical_info_b3_10\n",
      "[0 1]\n",
      "medical_info_b3_11\n",
      "[0 1]\n",
      "medical_info_b3_12\n",
      "[0 1]\n",
      "medical_info_b3_13\n",
      "[0 1]\n",
      "medical_info_b3_14\n",
      "[0 1]\n",
      "medical_info_b3_15\n",
      "[0 1]\n",
      "medical_info_b3_16\n",
      "[0 1]\n",
      "medical_info_c1\n",
      "[1.         1.49661416 2.         0.         3.         4.\n",
      " 5.         6.         7.         9.        ]\n",
      "medical_info_c2\n",
      "[15.00910673 16.16706537 19.90986162 ... 10.62650219 10.96315608\n",
      " 14.25848595]\n",
      "medical_keyword_1\n",
      "[1 0]\n",
      "medical_keyword_2\n",
      "[0 1]\n",
      "medical_keyword_3\n",
      "[0 1]\n",
      "medical_keyword_4\n",
      "[0 1]\n",
      "medical_keyword_5\n",
      "[0 1]\n",
      "medical_keyword_6\n",
      "[1 0]\n",
      "medical_keyword_7\n",
      "[0 1]\n",
      "medical_keyword_8\n",
      "[1 0]\n",
      "medical_keyword_9\n",
      "[0 1]\n",
      "medical_keyword_10\n",
      "[0 1]\n",
      "year\n",
      "[2015 2016]\n",
      "month\n",
      "[ 2  5  7  9 10  8 11  1 12  3  6  4]\n",
      "day\n",
      "[ 3  9 13  6 17 27 19  5  2 24 28 20 18 29  8 16 15  1 26  4 30 11 23 21\n",
      " 10 14  7 31 25 12 22]\n",
      "yearmonth\n",
      "[24182 24185 24194 24187 24201 24190 24188 24203 24193 24197 24204 24200\n",
      " 24183 24198 24181 24202 24192 24184 24195 24199 24196 24186 24189 24191]\n",
      "medical_info_c1_nan\n",
      "[False  True]\n",
      "medical_info_c2_nan\n",
      "[ True False]\n"
     ]
    }
   ],
   "source": [
    "for i in train_x1.columns:\n",
    "    print(i)\n",
    "    print( train_x1[i].unique() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.11.1 주성분 분석(PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-36ba64bc7c7a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# -----------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# 標準化されたデータを用いる\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_standarized_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m# -----------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-50-7ef022b10d0e>\u001b[0m in \u001b[0;36mload_standarized_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# numpy version 1.18 error발생\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[0mtrain_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mtest_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\front\\anaconda3\\envs\\kaggle\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    667\u001b[0m         \u001b[1;31m# Reset internal state before fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 669\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\front\\anaconda3\\envs\\kaggle\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    763\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_samples_seen_\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m                     _incremental_mean_and_var(X, self.mean_, self.var_,\n\u001b[1;32m--> 765\u001b[1;33m                                               self.n_samples_seen_)\n\u001b[0m\u001b[0;32m    766\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m         \u001b[1;31m# for backward-compatibility, reduce n_samples_seen_ to an integer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\front\\anaconda3\\envs\\kaggle\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36m_incremental_mean_and_var\u001b[1;34m(X, last_mean, last_variance, last_sample_count)\u001b[0m\n\u001b[0;32m    760\u001b[0m     \u001b[0mnew_sum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_safe_accumulator_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnansum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 762\u001b[1;33m     \u001b[0mnew_sample_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    763\u001b[0m     \u001b[0mupdated_sample_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlast_sample_count\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnew_sample_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# PCA\n",
    "# -----------------------------------\n",
    "# 標準化されたデータを用いる\n",
    "train_x, test_x = load_standarized_data()\n",
    "# -----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.11.1 은 에러 발생하여 이후부터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.11.2 NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# NMF\n",
    "# -----------------------------------\n",
    "# 非負の値とするため、MinMaxスケーリングを行ったデータを用いる\n",
    "train_x, test_x = load_minmax_scaled_data()\n",
    "# -----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# データは非負の値から構成されているとする\n",
    "\n",
    "# 学習データに基づいてNMFによる変換を定義\n",
    "model = NMF(n_components=5, init='random', random_state=71)\n",
    "model.fit(train_x)\n",
    "\n",
    "# 変換の適用\n",
    "train_x = model.transform(train_x)\n",
    "test_x = model.transform(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.11.3 Latent Dirichlet Allocation(LDA)\n",
    " * 에러 발생. 해결 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# LatentDirichletAllocation\n",
    "# -----------------------------------\n",
    "# MinMaxスケーリングを行ったデータを用いる\n",
    "# カウント行列ではないが、非負の値であれば計算は可能\n",
    "train_x, test_x = load_minmax_scaled_data()\n",
    "# -----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# データは単語文書のカウント行列などとする\n",
    "\n",
    "# 学習データに基づいてLDAによる変換を定義\n",
    "\n",
    "model = LatentDirichletAllocation(n_components=5, random_state=71)\n",
    "model.fit(train_x)\n",
    "\n",
    "# 変換の適用\n",
    "train_x = model.transform(train_x)\n",
    "test_x = model.transform(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.11.4 LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error 발생하여 추후 확인 필요\n",
    "```\n",
    "# -----------------------------------\n",
    "# LinearDiscriminantAnalysis\n",
    "# -----------------------------------\n",
    "# 標準化されたデータを用いる\n",
    "train_x, test_x = load_standarized_data()\n",
    "# -----------------------------------\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "# データは標準化などのスケールを揃える前処理が行われているものとする\n",
    "\n",
    "# 学習データに基づいてLDAによる変換を定義\n",
    "lda = LDA(n_components=1)\n",
    "lda.fit(train_x, train_y)\n",
    "\n",
    "# 変換の適用\n",
    "train_x = lda.transform(train_x)\n",
    "test_x = lda.transform(test_x)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.11.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-6f9bfd0211c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# -----------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# 標準化されたデータを用いる\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_standarized_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m# -----------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-50-7ef022b10d0e>\u001b[0m in \u001b[0;36mload_standarized_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# numpy version 1.18 error발생\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[0mtrain_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mtest_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\front\\anaconda3\\envs\\kaggle\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    667\u001b[0m         \u001b[1;31m# Reset internal state before fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 669\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\front\\anaconda3\\envs\\kaggle\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    763\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_samples_seen_\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m                     _incremental_mean_and_var(X, self.mean_, self.var_,\n\u001b[1;32m--> 765\u001b[1;33m                                               self.n_samples_seen_)\n\u001b[0m\u001b[0;32m    766\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m         \u001b[1;31m# for backward-compatibility, reduce n_samples_seen_ to an integer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\front\\anaconda3\\envs\\kaggle\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36m_incremental_mean_and_var\u001b[1;34m(X, last_mean, last_variance, last_sample_count)\u001b[0m\n\u001b[0;32m    760\u001b[0m     \u001b[0mnew_sum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_safe_accumulator_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnansum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 762\u001b[1;33m     \u001b[0mnew_sample_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    763\u001b[0m     \u001b[0mupdated_sample_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlast_sample_count\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnew_sample_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# t-sne\n",
    "# -----------------------------------\n",
    "# 標準化されたデータを用いる\n",
    "train_x, test_x = load_standarized_data()\n",
    "# -----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bhtsne\n",
    "\n",
    "# データは標準化などのスケールを揃える前処理が行われているものとする\n",
    "\n",
    "# t-sneによる変換\n",
    "data = pd.concat([train_x, test_x])\n",
    "embedded = bhtsne.tsne(data.astype(np.float64), dimensions=2, rand_seed=71)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# UMAP\n",
    "# -----------------------------------\n",
    "# 標準化されたデータを用いる\n",
    "train_x, test_x = load_standarized_data()\n",
    "# -----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "# データは標準化などのスケールを揃える前処理が行われているものとする\n",
    "\n",
    "# 学習データに基づいてUMAPによる変換を定義\n",
    "um = umap.UMAP()\n",
    "um.fit(train_x)\n",
    "\n",
    "# 変換の適用\n",
    "train_x = um.transform(train_x)\n",
    "test_x = um.transform(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.11.7 clustering(군집)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# クラスタリング\n",
    "# -----------------------------------\n",
    "# 標準化されたデータを用いる\n",
    "train_x, test_x = load_standarized_data()\n",
    "# -----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "# データは標準化などのスケールを揃える前処理が行われているものとする\n",
    "\n",
    "# 学習データに基づいてMini-Batch K-Meansによる変換を定義\n",
    "kmeans = MiniBatchKMeans(n_clusters=10, random_state=71)\n",
    "kmeans.fit(train_x)\n",
    "\n",
    "# 属するクラスタを出力する\n",
    "train_clusters = kmeans.predict(train_x)\n",
    "test_clusters = kmeans.predict(test_x)\n",
    "\n",
    "# 各クラスタの中心までの距離を出力する\n",
    "train_distances = kmeans.transform(train_x)\n",
    "test_distances = kmeans.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
