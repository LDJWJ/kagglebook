{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "personalized-trinidad",
   "metadata": {},
   "source": [
    "## 2.4 평가지표와 목적함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-venue",
   "metadata": {},
   "source": [
    "### 목차\n",
    " * 2.4.2 사용자 정의 평가지표와 사용자 정의 목적함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-motor",
   "metadata": {},
   "source": [
    "### 2.4.2 사용자 정의 평가지표와 사용자 정의 목적함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "formed-triangle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------\n",
    "# 데이터 등 준비\n",
    "# ----------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# train_x는 학습 데이터, train_y는 목적 변수, test_x는 테스트 데이터\n",
    "# pandas의 DataFrame, Series의 자료형 사용(numpy의 array로 값을 저장하기도 함.)\n",
    "train = pd.read_csv('../input/sample-data/train_preprocessed.csv')\n",
    "train_x = train.drop(['target'], axis=1)\n",
    "train_y = train['target']\n",
    "test_x = pd.read_csv('../input/sample-data/test_preprocessed.csv')\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=71)\n",
    "tr_idx, va_idx = list(kf.split(train_x))[0]\n",
    "\n",
    "# 학습 데이터를 학습 데이터와 평가용 데이터셋으로 분할\n",
    "tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n",
    "tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "continuous-animation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# xgboost에 있어, 사용자 평가지표와 목적 변수의 예\n",
    "# （참조）https://github.com/dmlc/xgboost/blob/master/demo/guide-python/custom_objective.py\n",
    "# -----------------------------------\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# 특징과 목적변수를 xgboost의 데이터 구조로 변환\n",
    "# 학습 데이터의 특징과 목적변수는 tr_x, tr_y\n",
    "# 검증 데이터의 특징과 목적변수는 va_x, va_y\n",
    "dtrain = xgb.DMatrix(tr_x, label=tr_y)\n",
    "dvalid = xgb.DMatrix(va_x, label=va_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rising-ethiopia",
   "metadata": {},
   "source": [
    "### 사용자 정의 목적함수 및 사용자 정의 평가지표"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "arbitrary-serial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자 정의 목적함수(이 경우는 logloss이며, xgboost의 ‘binary:logistic’과 동일)\n",
    "def logregobj(preds, dtrain):\n",
    "    labels = dtrain.get_label() # 실젯값 레이블 획득\n",
    "    preds = 1.0 / (1.0 + np.exp(-preds)) # 시그모이드 함수\n",
    "    grad = preds - labels # 그래디언트\n",
    "    hess = preds * (1.0 - preds) # 시그모이드 함수 미분\n",
    "    return grad, hess\n",
    "\n",
    "# 사용자 정의 평가지표(이 경우 오류율)\n",
    "def evalerror(preds, dtrain):\n",
    "    labels = dtrain.get_label() # 실젯값 레이블 획득\n",
    "    return 'custom-error', float(sum(labels != (preds > 0.0))) / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "pointed-qualification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터의 설정\n",
    "# xgboost 버전이 하위버전의 경우, 'verbosity':0을 'silent':1로 변경 후, 실행\n",
    "# params = {'silent': 1, 'random_state': 71}\n",
    "params = {'verbosity': 0, 'random_state': 71}   # xgboost 1.3.3 버전 적용\n",
    "num_round = 50\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monthly-jefferson",
   "metadata": {},
   "source": [
    "### 모델 학습 및 예측 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "massive-launch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.54088\ttrain-custom-error:0.12853\teval-logloss:0.55003\teval-custom-error:0.15160\n",
      "[1]\ttrain-logloss:0.45269\ttrain-custom-error:0.11533\teval-logloss:0.47182\teval-custom-error:0.14600\n",
      "[2]\ttrain-logloss:0.39482\ttrain-custom-error:0.10933\teval-logloss:0.42026\teval-custom-error:0.13760\n",
      "[3]\ttrain-logloss:0.35198\ttrain-custom-error:0.10533\teval-logloss:0.38520\teval-custom-error:0.13640\n",
      "[4]\ttrain-logloss:0.32021\ttrain-custom-error:0.09693\teval-logloss:0.36150\teval-custom-error:0.13840\n",
      "[5]\ttrain-logloss:0.29673\ttrain-custom-error:0.09467\teval-logloss:0.34463\teval-custom-error:0.13640\n",
      "[6]\ttrain-logloss:0.27610\ttrain-custom-error:0.08733\teval-logloss:0.32900\teval-custom-error:0.12960\n",
      "[7]\ttrain-logloss:0.25886\ttrain-custom-error:0.08493\teval-logloss:0.31670\teval-custom-error:0.12440\n",
      "[8]\ttrain-logloss:0.24363\ttrain-custom-error:0.07813\teval-logloss:0.30775\teval-custom-error:0.12080\n",
      "[9]\ttrain-logloss:0.23153\ttrain-custom-error:0.07373\teval-logloss:0.30092\teval-custom-error:0.11720\n",
      "[10]\ttrain-logloss:0.22016\ttrain-custom-error:0.06867\teval-logloss:0.29413\teval-custom-error:0.11600\n",
      "[11]\ttrain-logloss:0.20963\ttrain-custom-error:0.06493\teval-logloss:0.28528\teval-custom-error:0.11640\n",
      "[12]\ttrain-logloss:0.19951\ttrain-custom-error:0.06227\teval-logloss:0.27912\teval-custom-error:0.11120\n",
      "[13]\ttrain-logloss:0.19324\ttrain-custom-error:0.06053\teval-logloss:0.27642\teval-custom-error:0.11160\n",
      "[14]\ttrain-logloss:0.18547\ttrain-custom-error:0.05680\teval-logloss:0.27154\teval-custom-error:0.11120\n",
      "[15]\ttrain-logloss:0.17474\ttrain-custom-error:0.05040\teval-logloss:0.26516\teval-custom-error:0.10480\n",
      "[16]\ttrain-logloss:0.16900\ttrain-custom-error:0.04920\teval-logloss:0.26089\teval-custom-error:0.10040\n",
      "[17]\ttrain-logloss:0.16323\ttrain-custom-error:0.04640\teval-logloss:0.25849\teval-custom-error:0.10160\n",
      "[18]\ttrain-logloss:0.15950\ttrain-custom-error:0.04427\teval-logloss:0.25691\teval-custom-error:0.10280\n",
      "[19]\ttrain-logloss:0.15637\ttrain-custom-error:0.04347\teval-logloss:0.25511\teval-custom-error:0.10120\n",
      "[20]\ttrain-logloss:0.14722\ttrain-custom-error:0.03867\teval-logloss:0.25034\teval-custom-error:0.10160\n",
      "[21]\ttrain-logloss:0.14290\ttrain-custom-error:0.03653\teval-logloss:0.24734\teval-custom-error:0.10040\n",
      "[22]\ttrain-logloss:0.13782\ttrain-custom-error:0.03493\teval-logloss:0.24612\teval-custom-error:0.09960\n",
      "[23]\ttrain-logloss:0.13362\ttrain-custom-error:0.03373\teval-logloss:0.24387\teval-custom-error:0.10040\n",
      "[24]\ttrain-logloss:0.13047\ttrain-custom-error:0.03253\teval-logloss:0.24251\teval-custom-error:0.10400\n",
      "[25]\ttrain-logloss:0.12654\ttrain-custom-error:0.03120\teval-logloss:0.24094\teval-custom-error:0.10320\n",
      "[26]\ttrain-logloss:0.12268\ttrain-custom-error:0.02880\teval-logloss:0.24005\teval-custom-error:0.10200\n",
      "[27]\ttrain-logloss:0.11966\ttrain-custom-error:0.02773\teval-logloss:0.23803\teval-custom-error:0.10160\n",
      "[28]\ttrain-logloss:0.11506\ttrain-custom-error:0.02573\teval-logloss:0.23699\teval-custom-error:0.10040\n",
      "[29]\ttrain-logloss:0.11027\ttrain-custom-error:0.02320\teval-logloss:0.23626\teval-custom-error:0.10080\n",
      "[30]\ttrain-logloss:0.10827\ttrain-custom-error:0.02293\teval-logloss:0.23621\teval-custom-error:0.10240\n",
      "[31]\ttrain-logloss:0.10262\ttrain-custom-error:0.02067\teval-logloss:0.23269\teval-custom-error:0.09920\n",
      "[32]\ttrain-logloss:0.10061\ttrain-custom-error:0.01987\teval-logloss:0.23212\teval-custom-error:0.09920\n",
      "[33]\ttrain-logloss:0.09913\ttrain-custom-error:0.01973\teval-logloss:0.23180\teval-custom-error:0.09840\n",
      "[34]\ttrain-logloss:0.09582\ttrain-custom-error:0.01693\teval-logloss:0.23184\teval-custom-error:0.10280\n",
      "[35]\ttrain-logloss:0.09378\ttrain-custom-error:0.01733\teval-logloss:0.22998\teval-custom-error:0.10080\n",
      "[36]\ttrain-logloss:0.09243\ttrain-custom-error:0.01627\teval-logloss:0.22980\teval-custom-error:0.09880\n",
      "[37]\ttrain-logloss:0.08952\ttrain-custom-error:0.01520\teval-logloss:0.22913\teval-custom-error:0.09720\n",
      "[38]\ttrain-logloss:0.08732\ttrain-custom-error:0.01480\teval-logloss:0.22870\teval-custom-error:0.09800\n",
      "[39]\ttrain-logloss:0.08576\ttrain-custom-error:0.01453\teval-logloss:0.22786\teval-custom-error:0.09720\n",
      "[40]\ttrain-logloss:0.08340\ttrain-custom-error:0.01333\teval-logloss:0.22857\teval-custom-error:0.09760\n",
      "[41]\ttrain-logloss:0.08125\ttrain-custom-error:0.01253\teval-logloss:0.22695\teval-custom-error:0.09480\n",
      "[42]\ttrain-logloss:0.08027\ttrain-custom-error:0.01253\teval-logloss:0.22645\teval-custom-error:0.09560\n",
      "[43]\ttrain-logloss:0.07829\ttrain-custom-error:0.01147\teval-logloss:0.22659\teval-custom-error:0.09480\n",
      "[44]\ttrain-logloss:0.07616\ttrain-custom-error:0.01080\teval-logloss:0.22607\teval-custom-error:0.09680\n",
      "[45]\ttrain-logloss:0.07522\ttrain-custom-error:0.01093\teval-logloss:0.22499\teval-custom-error:0.09600\n",
      "[46]\ttrain-logloss:0.07313\ttrain-custom-error:0.01000\teval-logloss:0.22316\teval-custom-error:0.09320\n",
      "[47]\ttrain-logloss:0.07198\ttrain-custom-error:0.01027\teval-logloss:0.22293\teval-custom-error:0.09240\n",
      "[48]\ttrain-logloss:0.07025\ttrain-custom-error:0.00920\teval-logloss:0.22265\teval-custom-error:0.09120\n",
      "[49]\ttrain-logloss:0.06947\ttrain-custom-error:0.00947\teval-logloss:0.22226\teval-custom-error:0.09120\n",
      "0.6753993312120438\n",
      "[0]\ttrain-logloss:0.54088\teval-logloss:0.55003\n",
      "[1]\ttrain-logloss:0.45269\teval-logloss:0.47182\n",
      "[2]\ttrain-logloss:0.39482\teval-logloss:0.42026\n",
      "[3]\ttrain-logloss:0.35198\teval-logloss:0.38520\n",
      "[4]\ttrain-logloss:0.32021\teval-logloss:0.36150\n",
      "[5]\ttrain-logloss:0.29673\teval-logloss:0.34463\n",
      "[6]\ttrain-logloss:0.27610\teval-logloss:0.32900\n",
      "[7]\ttrain-logloss:0.25886\teval-logloss:0.31670\n",
      "[8]\ttrain-logloss:0.24363\teval-logloss:0.30775\n",
      "[9]\ttrain-logloss:0.23153\teval-logloss:0.30092\n",
      "[10]\ttrain-logloss:0.22016\teval-logloss:0.29413\n",
      "[11]\ttrain-logloss:0.20963\teval-logloss:0.28528\n",
      "[12]\ttrain-logloss:0.19951\teval-logloss:0.27912\n",
      "[13]\ttrain-logloss:0.19324\teval-logloss:0.27642\n",
      "[14]\ttrain-logloss:0.18547\teval-logloss:0.27154\n",
      "[15]\ttrain-logloss:0.17474\teval-logloss:0.26516\n",
      "[16]\ttrain-logloss:0.16900\teval-logloss:0.26089\n",
      "[17]\ttrain-logloss:0.16323\teval-logloss:0.25849\n",
      "[18]\ttrain-logloss:0.15950\teval-logloss:0.25691\n",
      "[19]\ttrain-logloss:0.15637\teval-logloss:0.25511\n",
      "[20]\ttrain-logloss:0.14722\teval-logloss:0.25034\n",
      "[21]\ttrain-logloss:0.14290\teval-logloss:0.24734\n",
      "[22]\ttrain-logloss:0.13782\teval-logloss:0.24612\n",
      "[23]\ttrain-logloss:0.13362\teval-logloss:0.24387\n",
      "[24]\ttrain-logloss:0.13047\teval-logloss:0.24251\n",
      "[25]\ttrain-logloss:0.12654\teval-logloss:0.24094\n",
      "[26]\ttrain-logloss:0.12268\teval-logloss:0.24005\n",
      "[27]\ttrain-logloss:0.11966\teval-logloss:0.23803\n",
      "[28]\ttrain-logloss:0.11506\teval-logloss:0.23699\n",
      "[29]\ttrain-logloss:0.11027\teval-logloss:0.23626\n",
      "[30]\ttrain-logloss:0.10827\teval-logloss:0.23621\n",
      "[31]\ttrain-logloss:0.10262\teval-logloss:0.23269\n",
      "[32]\ttrain-logloss:0.10061\teval-logloss:0.23212\n",
      "[33]\ttrain-logloss:0.09913\teval-logloss:0.23180\n",
      "[34]\ttrain-logloss:0.09582\teval-logloss:0.23184\n",
      "[35]\ttrain-logloss:0.09378\teval-logloss:0.22998\n",
      "[36]\ttrain-logloss:0.09243\teval-logloss:0.22980\n",
      "[37]\ttrain-logloss:0.08952\teval-logloss:0.22913\n",
      "[38]\ttrain-logloss:0.08732\teval-logloss:0.22870\n",
      "[39]\ttrain-logloss:0.08576\teval-logloss:0.22786\n",
      "[40]\ttrain-logloss:0.08340\teval-logloss:0.22857\n",
      "[41]\ttrain-logloss:0.08125\teval-logloss:0.22695\n",
      "[42]\ttrain-logloss:0.08027\teval-logloss:0.22645\n",
      "[43]\ttrain-logloss:0.07829\teval-logloss:0.22659\n",
      "[44]\ttrain-logloss:0.07616\teval-logloss:0.22607\n",
      "[45]\ttrain-logloss:0.07522\teval-logloss:0.22499\n",
      "[46]\ttrain-logloss:0.07313\teval-logloss:0.22316\n",
      "[47]\ttrain-logloss:0.07198\teval-logloss:0.22293\n",
      "[48]\ttrain-logloss:0.07025\teval-logloss:0.22265\n",
      "[49]\ttrain-logloss:0.06947\teval-logloss:0.22226\n",
      "0.22226432123804699\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습 실행\n",
    "bst = xgb.train(params, dtrain, num_round, watchlist, \n",
    "                obj=logregobj, feval=evalerror)\n",
    "\n",
    "# 목적함수에 binary:logistic을 지정했을 때와 달리 확률로 변환하기 전 값으로 \n",
    "# 예측값이 출력되므로 변환이 필요\n",
    "pred_val = bst.predict(dvalid)\n",
    "pred = 1.0 / (1.0 + np.exp(-pred_val))\n",
    "logloss = log_loss(va_y, pred)\n",
    "print(logloss)\n",
    "\n",
    "# (참고)일반적인 방법으로 학습하는 경우\n",
    "# params = {'silent': 1, 'random_state': 71, 'objective': 'binary:logistic'}\n",
    "params = {'verbosity': 0, 'random_state': 71, 'objective': 'binary:logistic'}   # 현 버전 1.3.0 버전\n",
    "\n",
    "bst = xgb.train(params, dtrain, num_round, watchlist)\n",
    "\n",
    "pred = bst.predict(dvalid)\n",
    "logloss = log_loss(va_y, pred)\n",
    "print(logloss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
